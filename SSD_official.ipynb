{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ5Rurh1rMRI"
   },
   "source": [
    "## SSD\n",
    ": proposal generation 부분을 제거하고 __Network에서 추출한 각기 다른 크기의 feature map에 다양한 크기와 비율을 가지는 default boxes를 도입__하여 속도와 성능 높인 1-shot 모델\n",
    "\n",
    "> cf) 1-stage Detector vs 2-stage Detector     \n",
    ">    \n",
    ">    \n",
    "> - 2-stage Detector는 물체의 위치를 찾는 __Localization 문제__와 문체를 식별하는 __Classification 문제__를 순차적으로 행하는 방법 (대표적으로 R-CNN계열)\n",
    "<img src='img/sort_2.png' width='500'>\n",
    "<center> 2-stage Detector (출처 : hoya012 .github.io)</center>\n",
    "<br>\n",
    "> - 1-stage Detector는 두 문제를 동시에 행하는 방법 (대표적으로 YOLO계열 SSD계열)\n",
    "<img src='img/sort_3.png' width='500'>\n",
    "<center> 1-stage Detector (출처 : hoya012 .github.io)</center>\n",
    "<br>\n",
    "> - 논문 흐름\n",
    "<img src='img/ssd_3.png' width='500'>\n",
    "        > : 2-stage detector(위) vs 1-stage detector(아래)\n",
    "<br>\n",
    "<br>\n",
    "> - 참고  \n",
    "> https://nuggy875.tistory.com/20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure  \n",
    "- Network에서 각기 다른 크기의 feature map을 추출 (이 부분이 YOLO와의 차이점)\n",
    "\n",
    "<img src='img/ssd_4.png' width='500'>  \n",
    "\n",
    "- 추출된 feature map에 대하여 localization과 confidence 구하기  \n",
    "\n",
    "<img src='img/ssd_6.png' width='300'>  \n",
    "\n",
    "> - 각 셀마다 3개의 default box가 만들어진다고 한다면, 5\\*5\\*256 feature map에 대해서는 5\\*5\\*3개의 default box가 만들어짐  \n",
    "> <br>\n",
    "> - 이때, localization의 output은 각 default에 대한 offset이므로 차원이 5\\*5\\*(4\\*3)가 되고, confidence의 output은 각 default에 대한 class confidence score이므로 전체 output의 차원이 5\\*5\\*(21\\*3)가 된다   \n",
    "> <br>\n",
    "> - 여기서, 21의 의미는 object의 class(20개) + background(1개)이다. 따라서 총 output의 차원은 5*5*3*(21+4)가 된다.  \n",
    "\n",
    "- Output\n",
    "<img src='img/ssd_7.png' width='500'>  \n",
    "> - default box(점선)에 해당하는 cell의 같은 index의 predicted bbox의 offset 갱신 및 confidence score 계산\n",
    "> - Confidence score가 설정한 threshold(0.01)이상인 것을 필터링하고 모든 feature map에 대하여 위의 과정을 반복한 뒤, NMS를 통해 객체를 예측\n",
    "\n",
    "- 참고 : https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=dr_moms&logNo=221656802353"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0YVRbJSOxkT"
   },
   "source": [
    "### Code\n",
    "\n",
    "```bash\n",
    "├── data\n",
    "├── layers\n",
    "│   ├── functions\n",
    "│   │   ├── detection.py\n",
    "│   │   └── prior_box.py\n",
    "│   ├── modules\n",
    "│   │   ├── l2norm.py\n",
    "│   │   └── mutlbox_loss.py\n",
    "│   └── box_utils.py\n",
    "├── utils\n",
    "│   │   └── augmentations.py\n",
    "├── eval.py\n",
    "├── ssd.py\n",
    "├── test.py\n",
    "└── train.py\n",
    "```\n",
    "https://github.com/amdegroot/ssd.pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bH-Oi_uOwoo"
   },
   "source": [
    "### ssd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HmRpWUOOwop"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from layers import *\n",
    "from data import voc, coco\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0xGLjHOOwoq"
   },
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    \"\"\"Single Shot Multibox Architecture\n",
    "    The network is composed of a base VGG network followed by the\n",
    "    added multibox conv layers.  Each multibox layer branches into\n",
    "        1) conv2d for class conf scores\n",
    "        2) conv2d for localization predictions\n",
    "        3) associated priorbox layer to produce default bounding\n",
    "           boxes specific to the layer's feature map size.\n",
    "    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    Args:\n",
    "        phase: (string) Can be \"test\" or \"train\"\n",
    "        size: input image size\n",
    "        base: VGG16 layers for input, size of either 300 or 500\n",
    "        extras: extra layers that feed to multibox loc and conf layers\n",
    "        head: \"multibox head\" consists of loc and conf conv layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, phase, size, base, extras, head, num_classes):\n",
    "        super(SSD, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.num_classes = num_classes\n",
    "        self.cfg = (coco, voc)[num_classes == 21]\n",
    "        self.priorbox = PriorBox(self.cfg)\n",
    "        self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
    "        self.size = size\n",
    "\n",
    "        # SSD network\n",
    "        self.vgg = nn.ModuleList(base)\n",
    "        \n",
    "        # Layer learns to scale the l2 normalized features from conv4_3\n",
    "        self.L2Norm = L2Norm(512, 20)\n",
    "        self.extras = nn.ModuleList(extras)\n",
    "\n",
    "        self.loc = nn.ModuleList(head[0])\n",
    "        self.conf = nn.ModuleList(head[1])\n",
    "\n",
    "        if phase == 'test':\n",
    "            self.softmax = nn.Softmax(dim=-1)\n",
    "            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies network layers and ops on input image(s) x.\n",
    "        Args:\n",
    "            x: input image or batch of images. Shape: [batch,3,300,300].\n",
    "        Return:\n",
    "            Depending on phase:\n",
    "            test:\n",
    "                Variable(tensor) of output class label predictions,\n",
    "                confidence score, and corresponding location predictions for\n",
    "                each object detected. Shape: [batch,topk,7]\n",
    "            train:\n",
    "                list of concat outputs from:\n",
    "                    1: confidence layers, Shape: [batch*num_priors, num_classes]\n",
    "                    2: localization layers, Shape: [batch, num_priors*4]\n",
    "                    3: priorbox layers, Shape: [2, num_priors*4]\n",
    "        \"\"\"\n",
    "        sources = list()\n",
    "        loc = list()\n",
    "        conf = list()\n",
    "\n",
    "        # apply vgg up to conv4_3 relu\n",
    "        for k in range(23):\n",
    "            x = self.vgg[k](x)\n",
    "\n",
    "        s = self.L2Norm(x)\n",
    "        sources.append(s)\n",
    "\n",
    "        # apply vgg up to fc7\n",
    "        for k in range(23, len(self.vgg)):\n",
    "            x = self.vgg[k](x)\n",
    "        sources.append(x)\n",
    "\n",
    "        # apply extra layers and cache source layer outputs\n",
    "        for k, v in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace=True)\n",
    "            if k % 2 == 1:\n",
    "                sources.append(x)\n",
    "\n",
    "        # apply multibox head to source layers\n",
    "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
    "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
    "\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1\n",
    "                         \n",
    "        if self.phase == \"test\":\n",
    "            output = self.detect(\n",
    "                loc.view(loc.size(0), -1, 4),                   # loc preds\n",
    "                self.softmax(conf.view(conf.size(0), -1,\n",
    "                             self.num_classes)),                # conf preds\n",
    "                self.priors.type(type(x.data))                  # default boxes\n",
    "            )\n",
    "                         \n",
    "        else:\n",
    "            output = (\n",
    "                loc.view(loc.size(0), -1, 4),\n",
    "                conf.view(conf.size(0), -1, self.num_classes),\n",
    "                self.priors\n",
    "            )\n",
    "                         \n",
    "        return output\n",
    "\n",
    "    def load_weights(self, base_file):\n",
    "        other, ext = os.path.splitext(base_file)\n",
    "        if ext == '.pkl' or '.pth':\n",
    "            print('Loading weights into state dict...')\n",
    "            self.load_state_dict(torch.load(base_file,\n",
    "                                 map_location=lambda storage, loc: storage))\n",
    "            print('Finished!')\n",
    "        else:\n",
    "            print('Sorry only .pth and .pkl files supported.')\n",
    "\n",
    "\n",
    "# This function is derived from torchvision VGG make_layers()\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "def vgg(cfg, i, batch_norm=False):\n",
    "                         \n",
    "    '''\n",
    "    vgg size에 따라 vgg model을 만들어주는 함수\n",
    "    input : \n",
    "        cfg : \n",
    "            아래의 base dictionary에 따른 layer list\n",
    "            아래는 size가 300인 경우의 input 값\n",
    "            input = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, \n",
    "                    'M', 512, 512, 512]\n",
    "        i : input channel\n",
    "    '''\n",
    "                         \n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        elif v == 'C':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6,\n",
    "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
    "    return layers\n",
    "\n",
    "\n",
    "def add_extras(cfg, i, batch_norm=False):\n",
    "    # Extra layers added to VGG for feature scaling\n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    flag = False\n",
    "    for k, v in enumerate(cfg):\n",
    "        if in_channels != 'S':\n",
    "            if v == 'S':\n",
    "                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n",
    "                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n",
    "            flag = not flag\n",
    "        in_channels = v\n",
    "    return layers\n",
    "\n",
    "                         \n",
    "def multibox(vgg, extra_layers, cfg, num_classes):\n",
    "    '''\n",
    "    vgg, extra_layers, cfg, num_classes를 입력으로 받아 최종모델을 리턴하는 함수\n",
    "    '''\n",
    "                         \n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "    vgg_source = [21, -2]\n",
    "    for k, v in enumerate(vgg_source):\n",
    "        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n",
    "                                 cfg[k] * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n",
    "                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n",
    "    for k, v in enumerate(extra_layers[1::2], 2):\n",
    "        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n",
    "                                 * 4, kernel_size=3, padding=1)]\n",
    "        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n",
    "                                  * num_classes, kernel_size=3, padding=1)]\n",
    "    return vgg, extra_layers, (loc_layers, conf_layers)\n",
    "\n",
    "\n",
    "base = {\n",
    "    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
    "            512, 512, 512],\n",
    "    '512': [],\n",
    "}\n",
    "extras = {\n",
    "    '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],\n",
    "    '512': [],\n",
    "}\n",
    "mbox = {\n",
    "    '300': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n",
    "    '512': [],\n",
    "}\n",
    "\n",
    "def build_ssd(phase, size=300, num_classes=21):\n",
    "    '''\n",
    "    phase, size 등을 입력으로 받아 SSD 모델 객체를 만들어주는 함수\n",
    "    '''\n",
    "                         \n",
    "    if phase != \"test\" and phase != \"train\":\n",
    "        print(\"ERROR: Phase: \" + phase + \" not recognized\")\n",
    "        return\n",
    "    if size != 300:\n",
    "        print(\"ERROR: You specified size \" + repr(size) + \". However, \" +\n",
    "              \"currently only SSD300 (size=300) is supported!\")\n",
    "        return\n",
    "    base_, extras_, head_ = multibox(vgg(base[str(size)], 3),\n",
    "                                     add_extras(extras[str(size)], 1024),\n",
    "                                     mbox[str(size)], num_classes)\n",
    "    return SSD(phase, size, base_, extras_, head_, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4ldmafEOwou"
   },
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoRucP5VOwow"
   },
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jmgo4PKlOwox"
   },
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Single Shot MultiBox Detector Training With Pytorch')\n",
    "train_set = parser.add_mutually_exclusive_group()\n",
    "parser.add_argument('--dataset', default='VOC', choices=['VOC', 'COCO'],\n",
    "                    type=str, help='VOC or COCO')\n",
    "parser.add_argument('--dataset_root', default=VOC_ROOT,\n",
    "                    help='Dataset root directory path')\n",
    "parser.add_argument('--basenet', default='vgg16_reducedfc.pth',\n",
    "                    help='Pretrained base model')\n",
    "parser.add_argument('--batch_size', default=32, type=int,\n",
    "                    help='Batch size for training')\n",
    "parser.add_argument('--resume', default=None, type=str,\n",
    "                    help='Checkpoint state_dict file to resume training from')\n",
    "parser.add_argument('--start_iter', default=0, type=int,\n",
    "                    help='Resume training at this iter')\n",
    "parser.add_argument('--num_workers', default=4, type=int,\n",
    "                    help='Number of workers used in dataloading')\n",
    "parser.add_argument('--cuda', default=True, type=str2bool,\n",
    "                    help='Use CUDA to train model')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                    help='Momentum value for optim')\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float,\n",
    "                    help='Weight decay for SGD')\n",
    "parser.add_argument('--gamma', default=0.1, type=float,\n",
    "                    help='Gamma update for SGD')\n",
    "parser.add_argument('--visdom', default=False, type=str2bool,\n",
    "                    help='Use visdom for loss visualization')\n",
    "parser.add_argument('--save_folder', default='weights/',\n",
    "                    help='Directory for saving checkpoint models')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_x3lQI8yOwox"
   },
   "outputs": [],
   "source": [
    "# cuda 이용가능여부에 따라 torch tensor type 설정해주기\n",
    "if torch.cuda.is_available():\n",
    "    if args.cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n",
    "              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "if not os.path.exists(args.save_folder):\n",
    "    os.mkdir(args.save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfwP0h-xOwoy"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    # Create dataset \n",
    "    if args.dataset == 'COCO':\n",
    "        if args.dataset_root == VOC_ROOT:\n",
    "            if not os.path.exists(COCO_ROOT):\n",
    "                parser.error('Must specify dataset_root if specifying dataset')\n",
    "            print(\"WARNING: Using default COCO dataset_root because \" +\n",
    "                  \"--dataset_root was not specified.\")\n",
    "            args.dataset_root = COCO_ROOT\n",
    "        cfg = coco\n",
    "        dataset = COCODetection(root=args.dataset_root,\n",
    "                                transform=SSDAugmentation(cfg['min_dim'],\n",
    "                                                          MEANS))\n",
    "    elif args.dataset == 'VOC':\n",
    "        if args.dataset_root == COCO_ROOT:\n",
    "            parser.error('Must specify dataset if specifying dataset_root')\n",
    "        cfg = voc\n",
    "        dataset = VOCDetection(root=args.dataset_root,\n",
    "                               transform=SSDAugmentation(cfg['min_dim'],\n",
    "                                                         MEANS))\n",
    "\n",
    "    if args.visdom:\n",
    "        import visdom\n",
    "        viz = visdom.Visdom()\n",
    "\n",
    "    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n",
    "    net = ssd_net\n",
    "\n",
    "    if args.cuda:\n",
    "        net = torch.nn.DataParallel(ssd_net)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    if args.resume:\n",
    "        print('Resuming training, loading {}...'.format(args.resume))\n",
    "        ssd_net.load_weights(args.resume)\n",
    "    else:\n",
    "        vgg_weights = torch.load(args.save_folder + args.basenet)\n",
    "        print('Loading base network...')\n",
    "        ssd_net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "    if args.cuda:\n",
    "        net = net.cuda()\n",
    "\n",
    "    if not args.resume:\n",
    "        print('Initializing weights...')\n",
    "        # initialize newly added layers' weights with xavier method\n",
    "        ssd_net.extras.apply(weights_init)\n",
    "        ssd_net.loc.apply(weights_init)\n",
    "        ssd_net.conf.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n",
    "                             False, args.cuda)\n",
    "\n",
    "    net.train()\n",
    "    # loss counters\n",
    "    loc_loss = 0\n",
    "    conf_loss = 0\n",
    "    epoch = 0\n",
    "    print('Loading the dataset...')\n",
    "\n",
    "    epoch_size = len(dataset) // args.batch_size\n",
    "    print('Training SSD on:', dataset.name)\n",
    "    print('Using the specified args:')\n",
    "    print(args)\n",
    "\n",
    "    step_index = 0\n",
    "\n",
    "    if args.visdom:\n",
    "        vis_title = 'SSD.PyTorch on ' + dataset.name\n",
    "        vis_legend = ['Loc Loss', 'Conf Loss', 'Total Loss']\n",
    "        iter_plot = create_vis_plot('Iteration', 'Loss', vis_title, vis_legend)\n",
    "        epoch_plot = create_vis_plot('Epoch', 'Loss', vis_title, vis_legend)\n",
    "\n",
    "    data_loader = data.DataLoader(dataset, args.batch_size,\n",
    "                                  num_workers=args.num_workers,\n",
    "                                  shuffle=True, collate_fn=detection_collate,\n",
    "                                  pin_memory=True)\n",
    "    # create batch iterator\n",
    "    batch_iterator = iter(data_loader)\n",
    "    for iteration in range(args.start_iter, cfg['max_iter']):\n",
    "        '''\n",
    "        args.start_iter : Resume training at this iter\n",
    "        '''\n",
    "        \n",
    "        if args.visdom and iteration != 0 and (iteration % epoch_size == 0):\n",
    "            update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None,\n",
    "                            'append', epoch_size)\n",
    "            # reset epoch loss counters\n",
    "            loc_loss = 0\n",
    "            conf_loss = 0\n",
    "            epoch += 1\n",
    "\n",
    "        # 지정한 step마다 lr 조정해주기\n",
    "        if iteration in cfg['lr_steps']:\n",
    "            step_index += 1\n",
    "            adjust_learning_rate(optimizer, args.gamma, step_index)\n",
    "\n",
    "        # load train data\n",
    "        images, targets = next(batch_iterator)\n",
    "\n",
    "        if args.cuda:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(ann, volatile=True) for ann in targets]\n",
    "            \n",
    "        # forward\n",
    "        t0 = time.time()\n",
    "        out = net(images)\n",
    "        \n",
    "        # backprop\n",
    "        optimizer.zero_grad()  # 역전파 전, gradient 0으로 초기화\n",
    "        loss_l, loss_c = criterion(out, targets)  # MultiBoxLoss 함수를 이용하여 loss 계산\n",
    "        \n",
    "            # MultiBoxLoss :\n",
    "            #  1) default boxes와의 IoU값이 threshold보다 큰 prior box를 찾고\n",
    "            #     그 prior box와 IoU값이 큰 gt boxes를 찾아서 matching\n",
    "            #  2) hard negative mining후, loss 계산해줌\n",
    "\n",
    "        loss = loss_l + loss_c\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t1 = time.time()\n",
    "        loc_loss += loss_l.data[0]\n",
    "        conf_loss += loss_c.data[0]\n",
    "\n",
    "        if iteration % 10 == 0:\n",
    "            print('timer: %.4f sec.' % (t1 - t0))\n",
    "            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data[0]), end=' ')\n",
    "\n",
    "        if args.visdom:\n",
    "            update_vis_plot(iteration, loss_l.data[0], loss_c.data[0],\n",
    "                            iter_plot, epoch_plot, 'append')\n",
    "\n",
    "        if iteration != 0 and iteration % 5000 == 0:\n",
    "            print('Saving state, iter:', iteration)\n",
    "            torch.save(ssd_net.state_dict(), 'weights/ssd300_COCO_' +\n",
    "                       repr(iteration) + '.pth')\n",
    "    torch.save(ssd_net.state_dict(),\n",
    "               args.save_folder + '' + args.dataset + '.pth')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    lr = args.lr * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def create_vis_plot(_xlabel, _ylabel, _title, _legend):\n",
    "    return viz.line(\n",
    "        X=torch.zeros((1,)).cpu(),\n",
    "        Y=torch.zeros((1, 3)).cpu(),\n",
    "        opts=dict(\n",
    "            xlabel=_xlabel,\n",
    "            ylabel=_ylabel,\n",
    "            title=_title,\n",
    "            legend=_legend\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def update_vis_plot(iteration, loc, conf, window1, window2, update_type,\n",
    "                    epoch_size=1):\n",
    "    viz.line(\n",
    "        X=torch.ones((1, 3)).cpu() * iteration,\n",
    "        Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu() / epoch_size,\n",
    "        win=window1,\n",
    "        update=update_type\n",
    "    )\n",
    "    \n",
    "    # initialize epoch plot on first iteration\n",
    "    if iteration == 0:\n",
    "        viz.line(\n",
    "            X=torch.zeros((1, 3)).cpu(),\n",
    "            Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu(),\n",
    "            win=window2,\n",
    "            update=True\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSoLdo3_Owo1"
   },
   "source": [
    "### eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwR7a7Oyqz2j"
   },
   "outputs": [],
   "source": [
    "\"\"\"Adapted from:\n",
    "    @longcw faster_rcnn_pytorch: https://github.com/longcw/faster_rcnn_pytorch\n",
    "    @rbgirshick py-faster-rcnn https://github.com/rbgirshick/py-faster-rcnn\n",
    "    Licensed under The MIT License [see LICENSE for details]\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from data import VOC_ROOT, VOCAnnotationTransform, VOCDetection, BaseTransform\n",
    "from data import VOC_CLASSES as labelmap\n",
    "import torch.utils.data as data\n",
    "\n",
    "from ssd import build_ssd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6zqO4Q8q4yM"
   },
   "outputs": [],
   "source": [
    "if sys.version_info[0] == 2:\n",
    "    import xml.etree.cElementTree as ET\n",
    "else:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Single Shot MultiBox Detector Evaluation')\n",
    "parser.add_argument('--trained_model',\n",
    "                    default='weights/ssd300_mAP_77.43_v2.pth', type=str,\n",
    "                    help='Trained state_dict file path to open')\n",
    "parser.add_argument('--save_folder', default='eval/', type=str,\n",
    "                    help='File path to save results')\n",
    "parser.add_argument('--confidence_threshold', default=0.01, type=float,\n",
    "                    help='Detection confidence threshold')\n",
    "parser.add_argument('--top_k', default=5, type=int,\n",
    "                    help='Further restrict the number of predictions to parse')\n",
    "parser.add_argument('--cuda', default=True, type=str2bool,\n",
    "                    help='Use cuda to train model')\n",
    "parser.add_argument('--voc_root', default=VOC_ROOT,\n",
    "                    help='Location of VOC root directory')\n",
    "parser.add_argument('--cleanup', default=True, type=str2bool,\n",
    "                    help='Cleanup and remove results files following eval')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not os.path.exists(args.save_folder):\n",
    "    os.mkdir(args.save_folder)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if args.cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: It looks like you have a CUDA device, but aren't using \\\n",
    "              CUDA.  Run with --cuda for optimal eval speed.\")\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "574e2mn7Owo3"
   },
   "outputs": [],
   "source": [
    "annopath = os.path.join(args.voc_root, 'VOC2007', 'Annotations', '%s.xml')\n",
    "imgpath = os.path.join(args.voc_root, 'VOC2007', 'JPEGImages', '%s.jpg')\n",
    "imgsetpath = os.path.join(args.voc_root, 'VOC2007', 'ImageSets',\n",
    "                          'Main', '{:s}.txt')\n",
    "YEAR = '2007'\n",
    "devkit_path = args.voc_root + 'VOC' + YEAR\n",
    "dataset_mean = (104, 117, 123)\n",
    "set_type = 'test'\n",
    "\n",
    "\n",
    "class Timer(object):\n",
    "    \"\"\"A simple timer.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_time = 0.\n",
    "        self.calls = 0\n",
    "        self.start_time = 0.\n",
    "        self.diff = 0.\n",
    "        self.average_time = 0.\n",
    "\n",
    "    def tic(self):\n",
    "        # using time.time instead of time.clock because time time.clock\n",
    "        # does not normalize for multithreading\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def toc(self, average=True):\n",
    "        self.diff = time.time() - self.start_time\n",
    "        self.total_time += self.diff\n",
    "        self.calls += 1\n",
    "        self.average_time = self.total_time / self.calls\n",
    "        if average:\n",
    "            return self.average_time\n",
    "        else:\n",
    "            return self.diff\n",
    "\n",
    "\n",
    "def parse_rec(filename):\n",
    "    \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "    tree = ET.parse(filename)\n",
    "    objects = []\n",
    "    for obj in tree.findall('object'):\n",
    "        obj_struct = {}\n",
    "        obj_struct['name'] = obj.find('name').text\n",
    "        obj_struct['pose'] = obj.find('pose').text\n",
    "        obj_struct['truncated'] = int(obj.find('truncated').text)\n",
    "        obj_struct['difficult'] = int(obj.find('difficult').text)\n",
    "        bbox = obj.find('bndbox')\n",
    "        obj_struct['bbox'] = [int(bbox.find('xmin').text) - 1,\n",
    "                              int(bbox.find('ymin').text) - 1,\n",
    "                              int(bbox.find('xmax').text) - 1,\n",
    "                              int(bbox.find('ymax').text) - 1]\n",
    "        objects.append(obj_struct)\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "def get_output_dir(name, phase):\n",
    "    \"\"\"Return the directory where experimental artifacts are placed.\n",
    "    If the directory does not exist, it is created.\n",
    "    A canonical path is built using the name from an imdb and a network\n",
    "    (if not None).\n",
    "    \"\"\"\n",
    "    filedir = os.path.join(name, phase)\n",
    "    if not os.path.exists(filedir):\n",
    "        os.makedirs(filedir)\n",
    "    return filedir\n",
    "\n",
    "\n",
    "def get_voc_results_file_template(image_set, cls):\n",
    "    # VOCdevkit/VOC2007/results/det_test_aeroplane.txt\n",
    "    filename = 'det_' + image_set + '_%s.txt' % (cls)\n",
    "    filedir = os.path.join(devkit_path, 'results')\n",
    "    if not os.path.exists(filedir):\n",
    "        os.makedirs(filedir)\n",
    "    path = os.path.join(filedir, filename)\n",
    "    return path\n",
    "\n",
    "\n",
    "def write_voc_results_file(all_boxes, dataset):\n",
    "    for cls_ind, cls in enumerate(labelmap):\n",
    "        print('Writing {:s} VOC results file'.format(cls))\n",
    "        filename = get_voc_results_file_template(set_type, cls)\n",
    "        with open(filename, 'wt') as f:\n",
    "            for im_ind, index in enumerate(dataset.ids):\n",
    "                dets = all_boxes[cls_ind+1][im_ind]\n",
    "                if dets == []:\n",
    "                    continue\n",
    "                # the VOCdevkit expects 1-based indices\n",
    "                for k in range(dets.shape[0]):\n",
    "                    f.write('{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n'.\n",
    "                            format(index[1], dets[k, -1],\n",
    "                                   dets[k, 0] + 1, dets[k, 1] + 1,\n",
    "                                   dets[k, 2] + 1, dets[k, 3] + 1))\n",
    "\n",
    "\n",
    "def do_python_eval(output_dir='output', use_07=True):\n",
    "    cachedir = os.path.join(devkit_path, 'annotations_cache')\n",
    "    aps = []\n",
    "    # The PASCAL VOC metric changed in 2010\n",
    "    use_07_metric = use_07\n",
    "    print('VOC07 metric? ' + ('Yes' if use_07_metric else 'No'))\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    for i, cls in enumerate(labelmap):\n",
    "        filename = get_voc_results_file_template(set_type, cls)\n",
    "        rec, prec, ap = voc_eval(\n",
    "           filename, annopath, imgsetpath.format(set_type), cls, cachedir,\n",
    "           ovthresh=0.5, use_07_metric=use_07_metric)\n",
    "        aps += [ap]\n",
    "        print('AP for {} = {:.4f}'.format(cls, ap))\n",
    "        with open(os.path.join(output_dir, cls + '_pr.pkl'), 'wb') as f:\n",
    "            pickle.dump({'rec': rec, 'prec': prec, 'ap': ap}, f)\n",
    "    print('Mean AP = {:.4f}'.format(np.mean(aps)))\n",
    "    print('~~~~~~~~')\n",
    "    print('Results:')\n",
    "    for ap in aps:\n",
    "        print('{:.3f}'.format(ap))\n",
    "    print('{:.3f}'.format(np.mean(aps)))\n",
    "    print('~~~~~~~~')\n",
    "    print('')\n",
    "    print('--------------------------------------------------------------')\n",
    "    print('Results computed with the **unofficial** Python eval code.')\n",
    "    print('Results should be very close to the official MATLAB eval code.')\n",
    "    print('--------------------------------------------------------------')\n",
    "\n",
    "\n",
    "def voc_ap(rec, prec, use_07_metric=True):\n",
    "    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n",
    "    Compute VOC AP given precision and recall.\n",
    "    If use_07_metric is true, uses the\n",
    "    VOC 07 11 point method (default:True).\n",
    "    \"\"\"\n",
    "    if use_07_metric:\n",
    "        # 11 point metric\n",
    "        ap = 0.\n",
    "        for t in np.arange(0., 1.1, 0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(prec[rec >= t])\n",
    "            ap = ap + p / 11.\n",
    "    else:\n",
    "        # correct AP calculation\n",
    "        # first append sentinel values at the end\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "        # compute the precision envelope\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "        # to calculate area under PR curve, look for points\n",
    "        # where X axis (recall) changes value\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "        # and sum (\\Delta recall) * prec\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "def voc_eval(detpath,\n",
    "             annopath,\n",
    "             imagesetfile,\n",
    "             classname,\n",
    "             cachedir,\n",
    "             ovthresh=0.5,\n",
    "             use_07_metric=True):\n",
    "    \"\"\"rec, prec, ap = voc_eval(detpath,\n",
    "                           annopath,\n",
    "                           imagesetfile,\n",
    "                           classname,\n",
    "                           [ovthresh],\n",
    "                           [use_07_metric])\n",
    "Top level function that does the PASCAL VOC evaluation.\n",
    "detpath: Path to detections\n",
    "   detpath.format(classname) should produce the detection results file.\n",
    "annopath: Path to annotations\n",
    "   annopath.format(imagename) should be the xml annotations file.\n",
    "imagesetfile: Text file containing the list of images, one image per line.\n",
    "classname: Category name (duh)\n",
    "cachedir: Directory for caching the annotations\n",
    "[ovthresh]: Overlap threshold (default = 0.5)\n",
    "[use_07_metric]: Whether to use VOC07's 11 point AP computation\n",
    "   (default True)\n",
    "\"\"\"\n",
    "# assumes detections are in detpath.format(classname)\n",
    "# assumes annotations are in annopath.format(imagename)\n",
    "# assumes imagesetfile is a text file with each line an image name\n",
    "# cachedir caches the annotations in a pickle file\n",
    "# first load gt\n",
    "    if not os.path.isdir(cachedir):\n",
    "        os.mkdir(cachedir)\n",
    "    cachefile = os.path.join(cachedir, 'annots.pkl')\n",
    "    # read list of images\n",
    "    with open(imagesetfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    imagenames = [x.strip() for x in lines]\n",
    "    if not os.path.isfile(cachefile):\n",
    "        # load annots\n",
    "        recs = {}\n",
    "        for i, imagename in enumerate(imagenames):\n",
    "            recs[imagename] = parse_rec(annopath % (imagename))\n",
    "            if i % 100 == 0:\n",
    "                print('Reading annotation for {:d}/{:d}'.format(\n",
    "                   i + 1, len(imagenames)))\n",
    "        # save\n",
    "        print('Saving cached annotations to {:s}'.format(cachefile))\n",
    "        with open(cachefile, 'wb') as f:\n",
    "            pickle.dump(recs, f)\n",
    "    else:\n",
    "        # load\n",
    "        with open(cachefile, 'rb') as f:\n",
    "            recs = pickle.load(f)\n",
    "\n",
    "    # extract gt objects for this class\n",
    "    class_recs = {}\n",
    "    npos = 0\n",
    "    for imagename in imagenames:\n",
    "        R = [obj for obj in recs[imagename] if obj['name'] == classname]\n",
    "        bbox = np.array([x['bbox'] for x in R])\n",
    "        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
    "        det = [False] * len(R)\n",
    "        npos = npos + sum(~difficult)\n",
    "        class_recs[imagename] = {'bbox': bbox,\n",
    "                                 'difficult': difficult,\n",
    "                                 'det': det}\n",
    "\n",
    "    # read dets\n",
    "    detfile = detpath.format(classname)\n",
    "    with open(detfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if any(lines) == 1:\n",
    "\n",
    "        splitlines = [x.strip().split(' ') for x in lines]\n",
    "        image_ids = [x[0] for x in splitlines]\n",
    "        confidence = np.array([float(x[1]) for x in splitlines])\n",
    "        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n",
    "\n",
    "        # sort by confidence\n",
    "        sorted_ind = np.argsort(-confidence)\n",
    "        sorted_scores = np.sort(-confidence)\n",
    "        BB = BB[sorted_ind, :]\n",
    "        image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "        # go down dets and mark TPs and FPs\n",
    "        nd = len(image_ids)\n",
    "        tp = np.zeros(nd)\n",
    "        fp = np.zeros(nd)\n",
    "        for d in range(nd):\n",
    "            R = class_recs[image_ids[d]]\n",
    "            bb = BB[d, :].astype(float)\n",
    "            ovmax = -np.inf\n",
    "            BBGT = R['bbox'].astype(float)\n",
    "            if BBGT.size > 0:\n",
    "                # compute overlaps\n",
    "                # intersection\n",
    "                ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "                iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "                ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "                iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "                iw = np.maximum(ixmax - ixmin, 0.)\n",
    "                ih = np.maximum(iymax - iymin, 0.)\n",
    "                inters = iw * ih\n",
    "                uni = ((bb[2] - bb[0]) * (bb[3] - bb[1]) +\n",
    "                       (BBGT[:, 2] - BBGT[:, 0]) *\n",
    "                       (BBGT[:, 3] - BBGT[:, 1]) - inters)\n",
    "                overlaps = inters / uni\n",
    "                ovmax = np.max(overlaps)\n",
    "                jmax = np.argmax(overlaps)\n",
    "\n",
    "            if ovmax > ovthresh:\n",
    "                if not R['difficult'][jmax]:\n",
    "                    if not R['det'][jmax]:\n",
    "                        tp[d] = 1.\n",
    "                        R['det'][jmax] = 1\n",
    "                    else:\n",
    "                        fp[d] = 1.\n",
    "            else:\n",
    "                fp[d] = 1.\n",
    "\n",
    "        # compute precision recall\n",
    "        fp = np.cumsum(fp)\n",
    "        tp = np.cumsum(tp)\n",
    "        rec = tp / float(npos)\n",
    "        # avoid divide by zero in case the first detection matches a difficult\n",
    "        # ground truth\n",
    "        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "        ap = voc_ap(rec, prec, use_07_metric)\n",
    "    else:\n",
    "        rec = -1.\n",
    "        prec = -1.\n",
    "        ap = -1.\n",
    "\n",
    "    return rec, prec, ap\n",
    "\n",
    "\n",
    "def test_net(save_folder, net, cuda, dataset, transform, top_k,\n",
    "             im_size=300, thresh=0.05):\n",
    "    num_images = len(dataset)\n",
    "    # all detections are collected into:\n",
    "    #    all_boxes[cls][image] = N x 5 array of detections in\n",
    "    #    (x1, y1, x2, y2, score)\n",
    "    all_boxes = [[[] for _ in range(num_images)]\n",
    "                 for _ in range(len(labelmap)+1)]\n",
    "\n",
    "    # timers\n",
    "    _t = {'im_detect': Timer(), 'misc': Timer()}\n",
    "    output_dir = get_output_dir('ssd300_120000', set_type)\n",
    "    det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "    for i in range(num_images):\n",
    "        im, gt, h, w = dataset.pull_item(i)\n",
    "\n",
    "        x = Variable(im.unsqueeze(0))\n",
    "        if args.cuda:\n",
    "            x = x.cuda()\n",
    "        _t['im_detect'].tic()\n",
    "        detections = net(x).data\n",
    "        detect_time = _t['im_detect'].toc(average=False)\n",
    "\n",
    "        # skip j = 0, because it's the background class\n",
    "        for j in range(1, detections.size(1)):\n",
    "            dets = detections[0, j, :]\n",
    "            mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()\n",
    "            dets = torch.masked_select(dets, mask).view(-1, 5)\n",
    "            if dets.size(0) == 0:\n",
    "                continue\n",
    "            boxes = dets[:, 1:]\n",
    "            boxes[:, 0] *= w\n",
    "            boxes[:, 2] *= w\n",
    "            boxes[:, 1] *= h\n",
    "            boxes[:, 3] *= h\n",
    "            scores = dets[:, 0].cpu().numpy()\n",
    "            cls_dets = np.hstack((boxes.cpu().numpy(),\n",
    "                                  scores[:, np.newaxis])).astype(np.float32,\n",
    "                                                                 copy=False)\n",
    "            all_boxes[j][i] = cls_dets\n",
    "\n",
    "        print('im_detect: {:d}/{:d} {:.3f}s'.format(i + 1,\n",
    "                                                    num_images, detect_time))\n",
    "\n",
    "    with open(det_file, 'wb') as f:\n",
    "        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Evaluating detections')\n",
    "    evaluate_detections(all_boxes, output_dir, dataset)\n",
    "\n",
    "\n",
    "def evaluate_detections(box_list, output_dir, dataset):\n",
    "    write_voc_results_file(box_list, dataset)\n",
    "    do_python_eval(output_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # load net\n",
    "    num_classes = len(labelmap) + 1                      # +1 for background\n",
    "    net = build_ssd('test', 300, num_classes)            # initialize SSD\n",
    "    net.load_state_dict(torch.load(args.trained_model))\n",
    "    net.eval()\n",
    "    print('Finished loading model!')\n",
    "    # load data\n",
    "    dataset = VOCDetection(args.voc_root, [('2007', set_type)],\n",
    "                           BaseTransform(300, dataset_mean),\n",
    "                           VOCAnnotationTransform())\n",
    "    if args.cuda:\n",
    "        net = net.cuda()\n",
    "        cudnn.benchmark = True\n",
    "    # evaluation\n",
    "    test_net(args.save_folder, net, args.cuda, dataset,\n",
    "             BaseTransform(net.size, dataset_mean), args.top_k, 300,\n",
    "             thresh=args.confidence_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT-M4JQMOwo5"
   },
   "source": [
    "### test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axdAwb1iOwo5"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from data import VOC_ROOT, VOC_CLASSES as labelmap\n",
    "from PIL import Image\n",
    "from data import VOCAnnotationTransform, VOCDetection, BaseTransform, VOC_CLASSES\n",
    "import torch.utils.data as data\n",
    "from ssd import build_ssd\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Single Shot MultiBox Detection')\n",
    "parser.add_argument('--trained_model', default='weights/ssd_300_VOC0712.pth',\n",
    "                    type=str, help='Trained state_dict file path to open')\n",
    "parser.add_argument('--save_folder', default='eval/', type=str,\n",
    "                    help='Dir to save results')\n",
    "parser.add_argument('--visual_threshold', default=0.6, type=float,\n",
    "                    help='Final confidence threshold')\n",
    "parser.add_argument('--cuda', default=True, type=bool,\n",
    "                    help='Use cuda to train model')\n",
    "parser.add_argument('--voc_root', default=VOC_ROOT, help='Location of VOC root directory')\n",
    "parser.add_argument('-f', default=None, type=str, help=\"Dummy arg so we can load in Jupyter Notebooks\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.cuda and torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "if not os.path.exists(args.save_folder):\n",
    "    os.mkdir(args.save_folder)\n",
    "\n",
    "\n",
    "def test_net(save_folder, net, cuda, testset, transform, thresh):\n",
    "    # dump predictions and assoc. ground truth to text file for now\n",
    "    filename = save_folder+'test1.txt'\n",
    "    num_images = len(testset)\n",
    "    for i in range(num_images):\n",
    "        print('Testing image {:d}/{:d}....'.format(i+1, num_images))\n",
    "        img = testset.pull_image(i)\n",
    "        img_id, annotation = testset.pull_anno(i)\n",
    "        x = torch.from_numpy(transform(img)[0]).permute(2, 0, 1)\n",
    "        x = Variable(x.unsqueeze(0))\n",
    "\n",
    "        with open(filename, mode='a') as f:\n",
    "            f.write('\\nGROUND TRUTH FOR: '+img_id+'\\n')\n",
    "            for box in annotation:\n",
    "                f.write('label: '+' || '.join(str(b) for b in box)+'\\n')\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "\n",
    "        y = net(x)      # forward pass\n",
    "        detections = y.data\n",
    "        # scale each detection back up to the image\n",
    "        scale = torch.Tensor([img.shape[1], img.shape[0],\n",
    "                             img.shape[1], img.shape[0]])\n",
    "        pred_num = 0\n",
    "        for i in range(detections.size(1)):\n",
    "            j = 0\n",
    "            while detections[0, i, j, 0] >= 0.6:\n",
    "                if pred_num == 0:\n",
    "                    with open(filename, mode='a') as f:\n",
    "                        f.write('PREDICTIONS: '+'\\n')\n",
    "                score = detections[0, i, j, 0]\n",
    "                label_name = labelmap[i-1]\n",
    "                pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n",
    "                coords = (pt[0], pt[1], pt[2], pt[3])\n",
    "                pred_num += 1\n",
    "                with open(filename, mode='a') as f:\n",
    "                    f.write(str(pred_num)+' label: '+label_name+' score: ' +\n",
    "                            str(score) + ' '+' || '.join(str(c) for c in coords) + '\\n')\n",
    "                j += 1\n",
    "\n",
    "\n",
    "def test_voc():\n",
    "    # load net\n",
    "    num_classes = len(VOC_CLASSES) + 1 # +1 background\n",
    "    net = build_ssd('test', 300, num_classes) # initialize SSD\n",
    "    net.load_state_dict(torch.load(args.trained_model))\n",
    "    net.eval()\n",
    "    print('Finished loading model!')\n",
    "    # load data\n",
    "    testset = VOCDetection(args.voc_root, [('2007', 'test')], None, VOCAnnotationTransform())\n",
    "    if args.cuda:\n",
    "        net = net.cuda()\n",
    "        cudnn.benchmark = True\n",
    "    # evaluation\n",
    "    test_net(args.save_folder, net, args.cuda, testset,\n",
    "             BaseTransform(net.size, (104, 117, 123)),\n",
    "             thresh=args.visual_threshold)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_voc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9PVqeV7QR6B"
   },
   "source": [
    "### layers folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxLPlVeNQ38D"
   },
   "source": [
    "#### box_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lYVqxxcQ6RD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def point_form(boxes):\n",
    "    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n",
    "    representation for comparison to point form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) center-size default boxes from priorbox layers.\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n",
    "\n",
    "\n",
    "def center_size(boxes):\n",
    "    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n",
    "    representation for comparison to center-size form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) point_form boxes\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n",
    "                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n",
    "\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = box_a.size(0)\n",
    "    B = box_b.size(0)\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "\n",
    "def jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
    "    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n",
    "    overlap, encode the bounding boxes, then return the matched indices\n",
    "    corresponding to both confidence and location preds.\n",
    "    Args:\n",
    "        threshold: (float) The overlap threshold used when mathing boxes.\n",
    "        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n",
    "        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n",
    "        variances: (tensor) Variances corresponding to each prior coord,\n",
    "            Shape: [num_priors, 4].\n",
    "        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n",
    "        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n",
    "        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n",
    "        idx: (int) current batch index\n",
    "    Return:\n",
    "        The matched indices corresponding to 1)location and 2)confidence preds.\n",
    "    \"\"\"\n",
    "    # IoU matrix 계산\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        point_form(priors)\n",
    "    )\n",
    "\n",
    "    # (Bipartite Matching)\n",
    "\n",
    "        # 각 gt boxes를 기준으로 가장 IoU값이 큰 prior boxes(default boxes)를 찾고\n",
    "        # 그 pred boxes와의 IoU값이 큰 gt boxes를 구하기 위해 아래와 같이\n",
    "        # 두 단계를 진행\n",
    "\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)  # max : 최댓값과 최댓값을 갖는 index 리턴\n",
    "        \n",
    "    # [1,num_priors] best ground truth for each prior\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "    \n",
    "    best_truth_idx.squeeze_(0)    # squeeze : 1차원인 축 제거\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n",
    "\n",
    "        # 각 each gt bbox에 대해서 가장 큰 prior의 idx에 2로 채워줌으로써\n",
    "        # 해당 index의 best_truth_overlap를 매우 크게 만들어줌 \n",
    "        # (IoU는 원래 1이하이므로 2로 만들어준 것은 다른 값보다 훨씬 크게 만들어 준 것)\n",
    "    \n",
    "    # TODO refactor: index  best_prior_idx with long tensor\n",
    "    # ensure every gt matches with its prior of max overlap\n",
    "    # j번째 best_prior_idx에 해당하는 best_truth_idx의 값을 확실히 j로 넣어줌!\n",
    "    for j in range(best_prior_idx.size(0)):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "\n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4] : matched gt boxes\n",
    "    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]   : class label of gt boxes\n",
    "    conf[best_truth_overlap < threshold] = 0  # label as background\n",
    "    loc = encode(matches, priors, variances)\n",
    "\n",
    "    # 현재 batch index에 loc, conf값을 넣어줌\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
    "\n",
    "\n",
    "def encode(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
    "    we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "\n",
    "    # dist b/t match center and prior's center (matched point와 prior point의 distance)\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh) / variances[1]\n",
    "    \n",
    "    # return target for smooth_l1_loss\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n",
    "\n",
    "\n",
    "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
    "def decode(loc, priors, variances):\n",
    "    \"\"\"Decode locations from predictions using priors to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        loc (tensor): location predictions for loc layers,\n",
    "            Shape: [num_priors,4]\n",
    "        priors (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        decoded bounding box predictions\n",
    "    \"\"\"\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
    "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    \"\"\"Utility function for computing log_sum_exp while determining\n",
    "    This will be used to determine unaveraged confidence loss across\n",
    "    all examples in a batch.\n",
    "    Args:\n",
    "        x (Variable(tensor)): conf_preds from conf layers\n",
    "    \"\"\"\n",
    "    x_max = x.data.max()\n",
    "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
    "\n",
    "\n",
    "# Original author: Francisco Massa:\n",
    "# https://github.com/fmassa/object-detection.torch\n",
    "# Ported to PyTorch by Max deGroot (02/01/2017)\n",
    "def nms(boxes, scores, overlap=0.5, top_k=200):\n",
    "    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n",
    "    overlapping bounding boxes for a given object.\n",
    "    Args:\n",
    "        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n",
    "        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n",
    "        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n",
    "        top_k: (int) The Maximum number of box preds to consider.\n",
    "    Return:\n",
    "        The indices of the kept boxes with respect to num_priors.\n",
    "    \"\"\"\n",
    "\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0:\n",
    "        return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    # I = I[v >= 0.01]\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    # keep = torch.Tensor()\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        # keep.append(i)\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1:\n",
    "            break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrSV8u7zQbhE"
   },
   "source": [
    "#### modules folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8Sro57LQebH"
   },
   "source": [
    "##### multibox_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcBJClK6Qj9W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from data import coco as cfg\n",
    "from ..box_utils import match, log_sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1623216735183,
     "user": {
      "displayName": "희지",
      "photoUrl": "",
      "userId": "04407225407236767854"
     },
     "user_tz": -540
    },
    "id": "-ytmCu9_QWJJ"
   },
   "outputs": [],
   "source": [
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    \n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "\n",
    "    Objective Loss:\n",
    "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
    "        weighted by α which is set to 1 by cross val.\n",
    "        Args:\n",
    "            c: class confidences,\n",
    "            l: predicted boxes,\n",
    "            g: ground truth boxes\n",
    "            N: number of matched default boxes\n",
    "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n",
    "                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n",
    "                 use_gpu=True):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching = prior_for_matching\n",
    "        self.do_neg_mining = neg_mining\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        self.variance = cfg['variance']\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"Multibox Loss\n",
    "        Args:\n",
    "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
    "            and prior boxes from SSD net.\n",
    "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
    "                loc shape: torch.size(batch_size,num_priors,4)\n",
    "                priors shape: torch.size(num_priors,4)\n",
    "            targets (tensor): Ground truth boxes and labels for a batch,\n",
    "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
    "        \"\"\"\n",
    "        loc_data, conf_data, priors = predictions\n",
    "        num = loc_data.size(0)\n",
    "        priors = priors[:loc_data.size(1), :]\n",
    "        num_priors = (priors.size(0))\n",
    "        num_classes = self.num_classes\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes (IoU 계산해서 매칭해줌)\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:, :-1].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "            match(self.threshold, truths, defaults, self.variance, labels,\n",
    "                  loc_t, conf_t, idx)\n",
    "        if self.use_gpu:\n",
    "            loc_t = loc_t.cuda()\n",
    "            conf_t = conf_t.cuda()\n",
    "\n",
    "        # wrap targets\n",
    "        loc_t = Variable(loc_t, requires_grad=False)\n",
    "        conf_t = Variable(conf_t, requires_grad=False)\n",
    "\n",
    "        pos = conf_t > 0\n",
    "        num_pos = pos.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Localization Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,4]\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c[pos] = 0  # filter out pos boxes for now\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "\n",
    "        N = num_pos.data.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        return loss_l, loss_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prior_box.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from math import sqrt as sqrt\n",
    "from itertools import product as product\n",
    "import torch\n",
    "\n",
    "\n",
    "class PriorBox(object):\n",
    "    \"\"\"Compute priorbox coordinates in center-offset form for each source\n",
    "    feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.image_size = cfg['min_dim']\n",
    "        # number of priors for feature map location (either 4 or 6)\n",
    "        self.num_priors = len(cfg['aspect_ratios'])\n",
    "        self.variance = cfg['variance'] or [0.1]\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.aspect_ratios = cfg['aspect_ratios']\n",
    "        self.clip = cfg['clip']\n",
    "        self.version = cfg['name']\n",
    "        for v in self.variance:\n",
    "            if v <= 0:\n",
    "                raise ValueError('Variances must be greater than 0')\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                \n",
    "                f_k = self.image_size / self.steps[k]  # size of k-th square feature map\n",
    "                \n",
    "                # unit center x,y\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: min_size\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: sqrt(s_k * s_(k+1))\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                # rest of aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "                    \n",
    "        # back to torch land\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SSD_official.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
