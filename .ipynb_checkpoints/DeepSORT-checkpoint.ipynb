{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSORT\n",
    "- SORT는 state estimation 불확실성이 낮을 때만 associate metric가 정확했기 때문에, occlusion을 잘 처리하지 못하였음. \n",
    "- 이를 해결하기 위해 SORT 알고리즘에 __(Motion &) Appearance information__을 추가한 모델\n",
    "- Longer periods of occlusions와 Identity switches 문제를 해결할 수 있었음\n",
    "- SORT에 사용된 recursive Kalman filtering과 frame-by-frame data association을 동일하게 사용하고 있음  \n",
    "\n",
    "### 1. Track Handling & State Estimation\n",
    "- Track Handling과 Kalman filtering은 SORT를 그대로 사용하고 있으나, state space가 약간 다름\n",
    "- 다음 프레임의 aspect ratio가 동일할 것이라고 가정한 SORT와 달리, DeepSORT는 8개의 차원을 사용하여 state space를 사용\n",
    "<img src='img/deepsort_1.png' width='150'>\n",
    "> - (u, v) : bounding box center position\n",
    "> - $\\gamma$ : aspect ratio\n",
    "> - h : height\n",
    "\n",
    "### 2. Assignment Problem\n",
    "- motion information : short-term predictions에 유용\n",
    "- appearance information은 long-term metrics에 유용\n",
    "- 두 정보의 가중합으로 cost metrics를 정의 (하지만, experiment를 해보니 $\\lambda=0$일 때가 성능이 좋았음. 즉, appearance information만 사용)\n",
    "<img src='img/deepsort_4.png' width='300'>\n",
    "<img src='img/deepsort_5.png' width='120'>\n",
    "\n",
    "#### 1) Motion information\n",
    "- predicted Kalman states와 newly arrived measurements의 Mahalanobis 거리를 사용\n",
    "- 역감마분포의 95% 신뢰수준에 해당하는 값을 threshold로 지정하여 가능성이 적은 association을 제거\n",
    "<img src='img/deepsort_2.png' width='250'>\n",
    "<img src='img/deepsort_3.png' width='200'>\n",
    "\n",
    "#### 2) Appearance information\n",
    "- i번째 track과 j번째 detection의 cosine distance를 사용\n",
    "- appearance vector를 구하기 위해서 pre-trained CNN를 사용  \n",
    "<img src='img/deepsort_6.png' width='300'>\n",
    "<img src='img/deepsort_7.png' width='120'>\n",
    "<img src='img/deepsort_8.png' width='300'>\n",
    "<center>pre-trained CNN architecture</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __cf) Mahalanobis distance & Cosine distance__   \n",
    "\n",
    "> <마할라노비스 거리(Mahalanobis distance)>\n",
    "- 마할라노비스 거리는 확률분포를 고려한 거리를 의미하여, 공식은 아래와 같다.  \n",
    "<img src='img/deepsort_10.png' width='250'>    \n",
    "\n",
    "\n",
    "> - 이때, S는 공분산 행렬을 의미하며, 공분산 행렬이 단위행렬인 경우, 유클리디안 거리와 같아진다.\n",
    "> - 좀더 직관적인 이해를 위해 예시를 들어 설명하고자 한다. 아래의 그림에서의 유클리디안 거리는 A-C가 A-B보다 긴 것을 확인할 수 있다. 하지만, 확률 분포를 생각해보면 C가 B에 비해서 A에 더 가까이 있을 확률이 크기 때문에 거리도 더 가깝다고 할 수 있다.  \n",
    "<img src='img/deepsort_11.png' width='220'>  \n",
    "\n",
    "\n",
    "> - 그렇다면 실제 마할라노비스 거리를 구해보자. 우선, 마할라노비스 거리를 구하기 위해 화이트닝 변환을 해주어야 한다. 이때, 화이트닝 변환이란 공분산 행렬이 단위 행렬이 되도록 벡터 x를 y로 변환하는 것을 의미하며, 변환 공식은 아래와 같다.  \n",
    "<img src='img/deepsort_12.png' width='150'>\n",
    "\n",
    "\n",
    "> - 위의 점들을 화이트닝을 변환을 해주면, 아래의 그림과 같아진다(파란 점들은 생략하였다). 공분산 행렬이 단위행렬이 되었기때문에 마할라노비스 거리는 유클리디안 거리와 같아지고, A-C의 거리가 A-B보다 더 짧아진 것을 확인할 수 있다.\n",
    "<img src='img/deepsort_13.png' width='100'>\n",
    "\n",
    "\n",
    "> <코사인 거리(Cosine Distance)>\n",
    "\n",
    "> - 코사인 거리는 두 벡터의 크기에 의존하지 않으며, 각도만 고려한 거리이며, 공식은 아래와 같다.  \n",
    "<img src='img/deepsort_14.png' width='250'>  \n",
    "\n",
    "\n",
    "> - 이때, Cosine Similiarity는 아래와 같이 정의된다.   \n",
    "<img src='img/deepsort_15.png' width='300'>\n",
    "\n",
    "> - 참고\n",
    "    - http://asq.kr/XnmkEk\n",
    "    - https://velog.io/@poiu8944/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DDistance-Metric-Types\n",
    "    - https://techblog-history-younghunjo1.tistory.com/84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Matching Cascade\n",
    "- 오랜 기간동안 occlusion이 일어나면 Kalman filter의 prediction이 uncertainty를 증가시킨다. 하지만, 직관적이지 않게도 Mahalanobis distance를 예시처럼 uncertainty를 선호한다. \n",
    "- 이를 해결하기 위해 __더 적은 age를 갖는 track에게 우선순위를 부여__하는 알고리즘을 사용하였다. 전체 알고리즘은 아래와 같다.  \n",
    "<img src='img/deepsort_9.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiements\n",
    "- SORT보다 __ID switches__를 45% 가량 줄였으며, 다른 online methods 중 가장 적은 ID switches를 나타낸다\n",
    "- 또한, longer occlusions에도 ID를 잘 유지할 수 있었다.\n",
    "- 하지만, __FP__의 수가 너무 많았다.  \n",
    "<img src='img/deepsort_16.png' width='600'>\n",
    "\n",
    "### 5. Conclustion\n",
    "\n",
    "- 본 논문에서는 SORT에 appearance information을 추가한 모델을 제시하였으며, 이로 인해 longer periods의 occlusion에도 잘 추적할 수 있었다. \n",
    "- 그럼에도 불구하고, 알고리즘이 단순하고 real-time으로 사용할 수 있으므로 현재까지 많이 사용되는 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBaMcSzMj8RY"
   },
   "source": [
    "## Code\n",
    "https://github.com/ZQPei/deep_sort_pytorch\n",
    "### yolov3_deepsort.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7ps4CmmkJr5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from detector import build_detector\n",
    "from deep_sort import build_tracker\n",
    "from utils.draw import draw_boxes\n",
    "from utils.parser import get_config\n",
    "from utils.log import get_logger\n",
    "from utils.io import write_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "171b0VfMj8EK"
   },
   "outputs": [],
   "source": [
    "class VideoTracker(object):\n",
    "    def __init__(self, cfg, args, video_path):\n",
    "        self.cfg = cfg\n",
    "        self.args = args\n",
    "        self.video_path = video_path\n",
    "        self.logger = get_logger(\"root\")\n",
    "\n",
    "        # gpu 사용여부\n",
    "        use_cuda = args.use_cuda and torch.cuda.is_available()\n",
    "        if not use_cuda:\n",
    "            warnings.warn(\"Running in cpu mode which maybe very slow!\", UserWarning)\n",
    "\n",
    "        # args.display이면 창을 띄워줌\n",
    "        if args.display:\n",
    "            cv2.namedWindow(\"test\", cv2.WINDOW_NORMAL)  \n",
    "                # cv2.namedWindow : 첫번째 인자(winname)라는 창을 만들어줌\n",
    "                # 두번째 인자(flag) : 창 옵션 \n",
    "                #   - cv2.WINDOW_NORMAL : 사용자가 창 크기 조장가능\n",
    "                #   - cv2.WINDOW_AUTOSIZE : 이미지와 동일한 크기로 크기 조정불가\n",
    "            cv2.resizeWindow(\"test\", args.display_width, args.display_height)\n",
    "                # cv2.resizeWindow : winname창의 크기를 (width, height)크기로 조정해줌\n",
    "\n",
    "        # 카메라 열기\n",
    "        # -1이면 VideoCapture 객체만 생성, -1이 아니면 해당 웹캠 열기\n",
    "        # (웹캠이 하나면 0, 2개 이상이면 첫 웹캠은 0, 두번째 웹캠은 1로 지정됨)\n",
    "        if args.cam != -1:\n",
    "            print(\"Using webcam \" + str(args.cam))\n",
    "            self.vdo = cv2.VideoCapture(args.cam)\n",
    "        else:\n",
    "            self.vdo = cv2.VideoCapture()\n",
    "\n",
    "        # detector, deepsort모델 build해주기\n",
    "        self.detector = build_detector(cfg, use_cuda=use_cuda) # YOLO v3 모델\n",
    "        self.deepsort = build_tracker(cfg, use_cuda=use_cuda)  # DeepSORT 모델\n",
    "        self.class_names = self.detector.class_names\n",
    "\n",
    "    def __enter__(self):\n",
    "        '''\n",
    "        __enter__ : with구문에 진입하는 시점에 자동으로 호출하는 메서드\n",
    "        '''\n",
    "\n",
    "        # args.cam이 -1이 아닌 경우, 프레임 받아오기\n",
    "        if self.args.cam != -1:\n",
    "            ret, frame = self.vdo.read()  \n",
    "                # 비디오의 한 프레임을 읽음\n",
    "                # return 값\n",
    "                #   - ret : 제대로 프레임을 읽으면 T, 실패하면 F\n",
    "                #   - fram : 읽은 프레임이 나옴\n",
    "\n",
    "            # 프레임을 제대로 읽지 못하면 Assertionerror 발생\n",
    "            assert ret, \"Error: Camera error\"  \n",
    "\n",
    "            # 프레임의 width, height 가져오기\n",
    "            self.im_width = frame.shape[0]\n",
    "            self.im_height = frame.shape[1]\n",
    "\n",
    "        # args.cam이 -1인 경우, video_path의 video open해주기\n",
    "        else:\n",
    "            assert os.path.isfile(self.video_path), \"Path error\"\n",
    "            self.vdo.open(self.video_path)\n",
    "            self.im_width = int(self.vdo.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            self.im_height = int(self.vdo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                # get : 항목 확인, set : 항목 변경\n",
    "                # cv2.CAP_PROP_FRAME_WIDTH : 프레임의 폭\n",
    "                # cv2.CAP_PROP_FRAME_HEIGHT : 프레임의 높이\n",
    "            assert self.vdo.isOpened()\n",
    "\n",
    "        # 결과를 저장해줄 dirs 만들어주기\n",
    "        if self.args.save_path:\n",
    "            os.makedirs(self.args.save_path, exist_ok=True)\n",
    "\n",
    "            # path of saved video and results\n",
    "            self.save_video_path = os.path.join(self.args.save_path, \"results.avi\")\n",
    "            self.save_results_path = os.path.join(self.args.save_path, \"results.txt\")\n",
    "\n",
    "            # create video writer\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "            self.writer = cv2.VideoWriter(self.save_video_path, fourcc, 20, (self.im_width, self.im_height))\n",
    "\n",
    "            # logging\n",
    "            self.logger.info(\"Save results to {}\".format(self.args.save_path))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        '''__exit__ : with 구문을 빠져나오기 직전에 호출되는 메소드'''\n",
    "        if exc_type:\n",
    "            print(exc_type, exc_value, exc_traceback)\n",
    "\n",
    "    def run(self):\n",
    "        results = []\n",
    "        idx_frame = 0\n",
    "\n",
    "        while self.vdo.grab():  # grab : 카메라 장치에 다음 프레임을 획득하라는 명령내리는 함수\n",
    "            idx_frame += 1\n",
    "            if idx_frame % self.args.frame_interval:  # frame_interval을 둬서 처리하기\n",
    "                continue\n",
    "\n",
    "            start = time.time()\n",
    "            _, ori_im = self.vdo.retrieve()  # retrieve : 획득한 프레임을 실제로 받아오는 함수\n",
    "            im = cv2.cvtColor(ori_im, cv2.COLOR_BGR2RGB)  \n",
    "                # BGR -> RGB (opencv는 색을 BGR로 처리하고 matplotlib에서는 RGB로 처리하기 때문에\n",
    "                # 바꿔줘야 색이 제대로 표시됨)\n",
    "\n",
    "            # do detection\n",
    "            bbox_xywh, cls_conf, cls_ids = self.detector(im)  # YOLO v3모델을 이용하여 detection하기 \n",
    "\n",
    "            # select person class\n",
    "            mask = cls_ids == 0\n",
    "\n",
    "            bbox_xywh = bbox_xywh[mask]\n",
    "            # bbox dilation just in case bbox too small, delete this line if using a better pedestrian detector\n",
    "            bbox_xywh[:, 3:] *= 1.2\n",
    "            cls_conf = cls_conf[mask]\n",
    "\n",
    "            # do tracking\n",
    "            outputs = self.deepsort.update(bbox_xywh, cls_conf, im)\n",
    "\n",
    "            # deepsort.update\n",
    "            #   1. confidence가 min_confidence보다 큰 detection에 대하여 NMS를 적용하고\n",
    "            #   2. 칼만필터를 이용하여 predict & update 진행\n",
    "            #       - Update : Matching, Measurement update, Distance Update (현재 active id에 대해서만 dist.)\n",
    "            #\n",
    "            #   Return : bbox & id\n",
    "\n",
    "            # draw boxes for visualization\n",
    "            if len(outputs) > 0:\n",
    "                bbox_tlwh = []\n",
    "                bbox_xyxy = outputs[:, :4]\n",
    "                identities = outputs[:, -1]\n",
    "                ori_im = draw_boxes(ori_im, bbox_xyxy, identities)\n",
    "\n",
    "                for bb_xyxy in bbox_xyxy:\n",
    "                    bbox_tlwh.append(self.deepsort._xyxy_to_tlwh(bb_xyxy))\n",
    "\n",
    "                results.append((idx_frame - 1, bbox_tlwh, identities))\n",
    "\n",
    "            end = time.time()\n",
    "\n",
    "            if self.args.display:\n",
    "                cv2.imshow(\"test\", ori_im)\n",
    "                cv2.waitKey(1)  # 1초가 지나고 다음 라인을 실행\n",
    "\n",
    "            if self.args.save_path:\n",
    "                self.writer.write(ori_im)\n",
    "\n",
    "            # save results\n",
    "            write_results(self.save_results_path, results, 'mot')\n",
    "\n",
    "            # logging\n",
    "            self.logger.info(\"time: {:.03f}s, fps: {:.03f}, detection numbers: {}, tracking numbers: {}\" \\\n",
    "                             .format(end - start, 1 / (end - start), bbox_xywh.shape[0], len(outputs)))\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"VIDEO_PATH\", type=str)\n",
    "    parser.add_argument(\"--config_detection\", type=str, default=\"./configs/yolov3.yaml\")\n",
    "    parser.add_argument(\"--config_deepsort\", type=str, default=\"./configs/deep_sort.yaml\")\n",
    "    # parser.add_argument(\"--ignore_display\", dest=\"display\", action=\"store_false\", default=True)\n",
    "    parser.add_argument(\"--display\", action=\"store_true\")\n",
    "    parser.add_argument(\"--frame_interval\", type=int, default=1)\n",
    "    parser.add_argument(\"--display_width\", type=int, default=800)\n",
    "    parser.add_argument(\"--display_height\", type=int, default=600)\n",
    "    parser.add_argument(\"--save_path\", type=str, default=\"./output/\")\n",
    "    parser.add_argument(\"--cpu\", dest=\"use_cuda\", action=\"store_false\", default=True)\n",
    "    parser.add_argument(\"--camera\", action=\"store\", dest=\"cam\", type=int, default=\"-1\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "    '''\n",
    "    add_argument 인자의 간단한 설명\n",
    "        action : 인자가 발견될 때의 수행할 액션의 기본형\n",
    "            - store(default) : 인자 값을 저장\n",
    "\n",
    "        dest : [parse_args()]가 반환하는 객체에 추가될 attribute의 이름\n",
    "            - 즉, dest='cam'으로 설정되면 args.cam으로 사용가능\n",
    "    '''\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    cfg = get_config()\n",
    "    cfg.merge_from_file(args.config_detection)\n",
    "    cfg.merge_from_file(args.config_deepsort)\n",
    "\n",
    "    with VideoTracker(cfg, args, video_path=args.VIDEO_PATH) as vdo_trk:\n",
    "        vdo_trk.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i0JeMdZj_sr"
   },
   "source": [
    "### deep_sort folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woTmVP9H_2u-"
   },
   "source": [
    "\n",
    "\n",
    "#### deep_sort.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeP-fSnp_6FH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from .deep.feature_extractor import Extractor\n",
    "from .sort.nn_matching import NearestNeighborDistanceMetric\n",
    "from .sort.preprocessing import non_max_suppression\n",
    "from .sort.detection import Detection\n",
    "from .sort.tracker import Tracker\n",
    "\n",
    "\n",
    "__all__ = ['DeepSort']\n",
    "\n",
    "\n",
    "class DeepSort(object):\n",
    "    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, nms_max_overlap=1.0, max_iou_distance=0.7, max_age=70, n_init=3, nn_budget=100, use_cuda=True):\n",
    "        self.min_confidence = min_confidence\n",
    "        self.nms_max_overlap = nms_max_overlap\n",
    "\n",
    "        self.extractor = Extractor(model_path, use_cuda=use_cuda)\n",
    "\n",
    "        max_cosine_distance = max_dist\n",
    "        nn_budget = 100\n",
    "        metric = NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "        self.tracker = Tracker(metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n",
    "\n",
    "    def update(self, bbox_xywh, confidences, ori_img):\n",
    "        self.height, self.width = ori_img.shape[:2]\n",
    "        \n",
    "        # generate detections\n",
    "        features = self._get_features(bbox_xywh, ori_img)  \n",
    "        bbox_tlwh = self._xywh_to_tlwh(bbox_xywh)    \n",
    "        detections = [Detection(bbox_tlwh[i], conf, features[i]) for i, conf in enumerate(confidences) if conf > self.min_confidence]\n",
    "            # confidence가 min_confidence보다 큰 detections만 추출\n",
    "\n",
    "        # detections list에 대하여 NMS(non-maximum supression) 적용\n",
    "        boxes = np.array([d.tlwh for d in detections])\n",
    "        scores = np.array([d.confidence for d in detections])\n",
    "        indices = non_max_suppression(boxes, self.nms_max_overlap, scores)\n",
    "        detections = [detections[i] for i in indices]\n",
    "\n",
    "        # update tracker\n",
    "        self.tracker.predict()  # kalman filter의 predict부분 (다음 상태 예측값 및 예측공분산 계산)\n",
    "        self.tracker.update(detections)  # Matching, Measurement update, Distance metrics update\n",
    "\n",
    "\n",
    "            # tracker.update : \n",
    "            #   1) Matching Cascade에 따라 Confirmed된 Track(처음 등장후, n_init이상 프레임에서 detection된 tracker)과 \n",
    "            #      Detection을 Matching\n",
    "            #   2) Unconfirmed track 중 time_since_update가 안 된 track과 unmatched detection을 matching\n",
    "            #   3) Track update(measurement update)\n",
    "            #   4) distance metrics 업데이트 (현재 프레임에 있는 active target id에 대해서만 계산된 distance metrix로 update)\n",
    "\n",
    "            # ** 매칭시, cost matrix를 cosine dist. of appearance vector로 정의하였고,\n",
    "            #    indicator를 이용해 (predicted kalman state와 detection) Mahalanobis dist.가 일정값 이상인경우 또는\n",
    "            #    cosine dist.가 일정값 이상인 경우를 제외시켜주었음.\n",
    "\n",
    "        # output bbox and identities (box 좌표와 id를 output)\n",
    "        outputs = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            box = track.to_tlwh()\n",
    "            x1,y1,x2,y2 = self._tlwh_to_xyxy(box)\n",
    "            track_id = track.track_id\n",
    "            outputs.append(np.array([x1,y1,x2,y2,track_id], dtype=np.int))\n",
    "        if len(outputs) > 0:\n",
    "            outputs = np.stack(outputs,axis=0)\n",
    "        return outputs\n",
    "\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Convert bbox from xc_yc_w_h to xtl_ytl_w_h\n",
    "        즉, (center_x, center_y, w, h) -> (x_topleft, y_topleft, w, h) 변환\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def _xywh_to_tlwh(bbox_xywh):\n",
    "        if isinstance(bbox_xywh, np.ndarray):\n",
    "            bbox_tlwh = bbox_xywh.copy()\n",
    "        elif isinstance(bbox_xywh, torch.Tensor):\n",
    "            bbox_tlwh = bbox_xywh.clone()\n",
    "        bbox_tlwh[:,0] = bbox_xywh[:,0] - bbox_xywh[:,2]/2.\n",
    "        bbox_tlwh[:,1] = bbox_xywh[:,1] - bbox_xywh[:,3]/2.\n",
    "        return bbox_tlwh\n",
    "\n",
    "\n",
    "    def _xywh_to_xyxy(self, bbox_xywh):\n",
    "        x,y,w,h = bbox_xywh\n",
    "        x1 = max(int(x-w/2),0)\n",
    "        x2 = min(int(x+w/2),self.width-1)\n",
    "        y1 = max(int(y-h/2),0)\n",
    "        y2 = min(int(y+h/2),self.height-1)\n",
    "        return x1,y1,x2,y2\n",
    "\n",
    "    def _tlwh_to_xyxy(self, bbox_tlwh):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            Convert bbox from xtl_ytl_w_h to xc_yc_w_h\n",
    "        Thanks JieChen91@github.com for reporting this bug!\n",
    "        \"\"\"\n",
    "        x,y,w,h = bbox_tlwh\n",
    "        x1 = max(int(x),0)\n",
    "        x2 = min(int(x+w),self.width-1)\n",
    "        y1 = max(int(y),0)\n",
    "        y2 = min(int(y+h),self.height-1)\n",
    "        return x1,y1,x2,y2\n",
    "\n",
    "    def _xyxy_to_tlwh(self, bbox_xyxy):\n",
    "        x1,y1,x2,y2 = bbox_xyxy\n",
    "\n",
    "        t = x1\n",
    "        l = y1\n",
    "        w = int(x2-x1)\n",
    "        h = int(y2-y1)\n",
    "        return t,l,w,h\n",
    "    \n",
    "    def _get_features(self, bbox_xywh, ori_img):\n",
    "        im_crops = []\n",
    "        for box in bbox_xywh:\n",
    "            x1,y1,x2,y2 = self._xywh_to_xyxy(box)\n",
    "            im = ori_img[y1:y2,x1:x2]\n",
    "            im_crops.append(im)\n",
    "        if im_crops:\n",
    "            features = self.extractor(im_crops)\n",
    "        else:\n",
    "            features = np.array([])\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0rfXqgKweNT"
   },
   "source": [
    "#### deep folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-S14Mcp1AyR"
   },
   "source": [
    "##### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28ASIpRQ1BGp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out,is_downsample=False):\n",
    "\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.is_downsample = is_downsample\n",
    "        if is_downsample:\n",
    "            self.conv1 = nn.Conv2d(c_in, c_out, 3, stride=2, padding=1, bias=False)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(c_in, c_out, 3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(c_out)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.conv2 = nn.Conv2d(c_out,c_out,3,stride=1,padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(c_out)\n",
    "\n",
    "        if is_downsample:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(c_in, c_out, 1, stride=2, bias=False),\n",
    "                nn.BatchNorm2d(c_out)\n",
    "            )\n",
    "        elif c_in != c_out:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(c_in, c_out, 1, stride=1, bias=False),\n",
    "                nn.BatchNorm2d(c_out)\n",
    "            )\n",
    "            self.is_downsample = True\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        if self.is_downsample:\n",
    "            x = self.downsample(x)\n",
    "        return F.relu(x.add(y),True)\n",
    "\n",
    "def make_layers(c_in,c_out,repeat_times, is_downsample=False):\n",
    "    blocks = []\n",
    "    for i in range(repeat_times):\n",
    "        if i ==0:\n",
    "            blocks += [BasicBlock(c_in,c_out, is_downsample=is_downsample),]\n",
    "        else:\n",
    "            blocks += [BasicBlock(c_out,c_out),]\n",
    "    return nn.Sequential(*blocks)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=751 ,reid=False):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        # 3 128 64\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3,64,3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Conv2d(32,32,3,stride=1,padding=1),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3,2,padding=1),\n",
    "        )\n",
    "\n",
    "        # 32 64 32\n",
    "        self.layer1 = make_layers(64,64,2,False)\n",
    "        # 32 64 32\n",
    "        self.layer2 = make_layers(64,128,2,True)\n",
    "        # 64 32 16\n",
    "        self.layer3 = make_layers(128,256,2,True)\n",
    "        # 128 16 8\n",
    "        self.layer4 = make_layers(256,512,2,True)\n",
    "        # 256 8 4\n",
    "        self.avgpool = nn.AvgPool2d((8,4),1)\n",
    "        # 256 1 1 \n",
    "        self.reid = reid\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        # B x 128\n",
    "        if self.reid:\n",
    "            x = x.div(x.norm(p=2,dim=1,keepdim=True))\n",
    "            return x\n",
    "        # classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = Net()\n",
    "    x = torch.randn(4,3,128,64)\n",
    "    y = net(x)\n",
    "    import ipdb; ipdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RZP_a7hgnbl"
   },
   "source": [
    "##### feature_extractor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RYHtjZGgntA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import logging\n",
    "\n",
    "from .model import Net\n",
    "\n",
    "class Extractor(object):\n",
    "    def __init__(self, model_path, use_cuda=True):\n",
    "        self.net = Net(reid=True)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
    "        state_dict = torch.load(model_path, map_location=lambda storage, loc: storage)['net_dict']\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        logger = logging.getLogger(\"root.tracker\")\n",
    "        logger.info(\"Loading weights from {}... Done!\".format(model_path))\n",
    "        self.net.to(self.device)\n",
    "        self.size = (64, 128)\n",
    "        self.norm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def _preprocess(self, im_crops):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. to float with scale from 0 to 1\n",
    "            2. resize to (64, 128) as Market1501 dataset did\n",
    "            3. concatenate to a numpy array\n",
    "            3. to torch Tensor\n",
    "            4. normalize\n",
    "        \"\"\"\n",
    "        def _resize(im, size):\n",
    "            return cv2.resize(im.astype(np.float32)/255., size)\n",
    "\n",
    "        im_batch = torch.cat([self.norm(_resize(im, self.size)).unsqueeze(0) for im in im_crops], dim=0).float()\n",
    "        return im_batch\n",
    "\n",
    "    \n",
    "    def __call__(self, im_crops):\n",
    "        '''\n",
    "        Extractor가 호출되었을 때 실행되는 함수\n",
    "        - 전처리 및 forward 수행\n",
    "        - pretrained network를 사용하는 것이므로 grad 계산 X (training X)\n",
    "        '''\n",
    "\n",
    "        im_batch = self._preprocess(im_crops)\n",
    "        with torch.no_grad():\n",
    "            im_batch = im_batch.to(self.device)\n",
    "            features = self.net(im_batch)\n",
    "        return features.cpu().numpy()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img = cv2.imread(\"demo.jpg\")[:,:,(2,1,0)]\n",
    "    extr = Extractor(\"checkpoint/ckpt.t7\")\n",
    "    feature = extr(img)\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekCvF4yC09nO"
   },
   "source": [
    "\n",
    "##### train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJl6-s2UzWIE"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "\n",
    "from model import Net\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Train on market1501\")\n",
    "parser.add_argument(\"--data-dir\",default='data',type=str)\n",
    "parser.add_argument(\"--no-cuda\",action=\"store_true\")\n",
    "parser.add_argument(\"--gpu-id\",default=0,type=int)\n",
    "parser.add_argument(\"--lr\",default=0.1, type=float)\n",
    "parser.add_argument(\"--interval\",'-i',default=20,type=int)\n",
    "parser.add_argument('--resume', '-r',action='store_true')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# device\n",
    "device = \"cuda:{}\".format(args.gpu_id) if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "if torch.cuda.is_available() and not args.no_cuda:\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQLZYXr1verB"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "root = args.data_dir\n",
    "train_dir = os.path.join(root,\"train\")\n",
    "test_dir = os.path.join(root,\"test\")\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop((128,64),padding=4),  \n",
    "       # 128*64 size로 random하게 crop, 상하좌우 4영역에 대하여 padding (default는 0으로 padding됨)\n",
    "    torchvision.transforms.RandomHorizontalFlip(),  # 좌우반전\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        # 주어진 mean, std를 갖는 image를 정규화\n",
    "\n",
    "])\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((128,64)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_dir, transform=transform_train),\n",
    "    batch_size=64,shuffle=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_dir, transform=transform_test),\n",
    "    batch_size=64,shuffle=True\n",
    ")\n",
    "num_classes = max(len(trainloader.dataset.classes), len(testloader.dataset.classes))\n",
    "\n",
    "# net definition\n",
    "start_epoch = 0\n",
    "net = Net(num_classes=num_classes)\n",
    "if args.resume:\n",
    "    assert os.path.isfile(\"./checkpoint/ckpt.t7\"), \"Error: no checkpoint file found!\"\n",
    "    print('Loading from checkpoint/ckpt.t7')\n",
    "    checkpoint = torch.load(\"./checkpoint/ckpt.t7\")\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    net_dict = checkpoint['net_dict']\n",
    "    net.load_state_dict(net_dict)\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "net.to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "best_acc = 0.\n",
    "\n",
    "# train function for each epoch\n",
    "# appearance vector를 훈련하는 함수\n",
    "def train(epoch):\n",
    "    print(\"\\nEpoch : %d\"%(epoch+1))\n",
    "    net.train()\n",
    "    training_loss = 0.\n",
    "    train_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    interval = args.interval\n",
    "    start = time.time()\n",
    "    \n",
    "    for idx, (inputs, labels) in enumerate(trainloader):\n",
    "        # forward\n",
    "        inputs,labels = inputs.to(device),labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumurating\n",
    "        training_loss += loss.item()\n",
    "        train_loss += loss.item()\n",
    "        correct += outputs.max(dim=1)[1].eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # print \n",
    "        if (idx+1)%interval == 0:\n",
    "            end = time.time()\n",
    "            print(\"[progress:{:.1f}%]time:{:.2f}s Loss:{:.5f} Correct:{}/{} Acc:{:.3f}%\".format(\n",
    "                100.*(idx+1)/len(trainloader), end-start, training_loss/interval, correct, total, 100.*correct/total\n",
    "            ))\n",
    "            training_loss = 0.\n",
    "            start = time.time()\n",
    "    \n",
    "    return train_loss/len(trainloader), 1.- correct/total\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(testloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            correct += outputs.max(dim=1)[1].eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        print(\"Testing ...\")\n",
    "        end = time.time()\n",
    "        print(\"[progress:{:.1f}%]time:{:.2f}s Loss:{:.5f} Correct:{}/{} Acc:{:.3f}%\".format(\n",
    "                100.*(idx+1)/len(testloader), end-start, test_loss/len(testloader), correct, total, 100.*correct/total\n",
    "            ))\n",
    "\n",
    "    # saving checkpoint\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        print(\"Saving parameters to checkpoint/ckpt.t7\")\n",
    "        checkpoint = {\n",
    "            'net_dict':net.state_dict(),\n",
    "            'acc':acc,\n",
    "            'epoch':epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(checkpoint, './checkpoint/ckpt.t7')\n",
    "\n",
    "    return test_loss/len(testloader), 1.- correct/total\n",
    "\n",
    "# plot figure\n",
    "x_epoch = []\n",
    "record = {'train_loss':[], 'train_err':[], 'test_loss':[], 'test_err':[]}\n",
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121, title=\"loss\")\n",
    "ax1 = fig.add_subplot(122, title=\"top1err\")\n",
    "def draw_curve(epoch, train_loss, train_err, test_loss, test_err):\n",
    "    global record\n",
    "    record['train_loss'].append(train_loss)\n",
    "    record['train_err'].append(train_err)\n",
    "    record['test_loss'].append(test_loss)\n",
    "    record['test_err'].append(test_err)\n",
    "\n",
    "    x_epoch.append(epoch)\n",
    "    ax0.plot(x_epoch, record['train_loss'], 'bo-', label='train')\n",
    "    ax0.plot(x_epoch, record['test_loss'], 'ro-', label='val')\n",
    "    ax1.plot(x_epoch, record['train_err'], 'bo-', label='train')\n",
    "    ax1.plot(x_epoch, record['test_err'], 'ro-', label='val')\n",
    "    if epoch == 0:\n",
    "        ax0.legend()\n",
    "        ax1.legend()\n",
    "    fig.savefig(\"train.jpg\")\n",
    "\n",
    "# lr decay\n",
    "def lr_decay():\n",
    "    global optimizer\n",
    "    for params in optimizer.param_groups:\n",
    "        params['lr'] *= 0.1\n",
    "        lr = params['lr']\n",
    "        print(\"Learning rate adjusted to {}\".format(lr))\n",
    "\n",
    "def main():\n",
    "    for epoch in range(start_epoch, start_epoch+40):\n",
    "        train_loss, train_err = train(epoch)\n",
    "        test_loss, test_err = test(epoch)\n",
    "        draw_curve(epoch, train_loss, train_err, test_loss, test_err)\n",
    "        if (epoch+1)%20==0:\n",
    "            lr_decay()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMUndIJVWOjNWd5lrvmDQST",
   "collapsed_sections": [],
   "name": "DeepSORT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
