{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiamFC \n",
    "https://arxiv.org/pdf/1606.09549v2.pdf  \n",
    ": fully-convolutional Siamese network를 오프라인으로 훈련시켜 FPS를 높인 모델\n",
    "\n",
    "### 0. Abstract\n",
    "- 2016년에 나온 모델로, 이전의 모델들은 online으로 object의 appearance를 학습하는 모델을 훈련시켜 한계가 있었음.\n",
    "- 이를 해결하기 위해 end-to-end로 훈련될 수 있는 novel fully-convolutional Siamese network를 제안함\n",
    "\n",
    "### 1. Introduction\n",
    "- 더 큰 search image에서 exemplar image의 위치를 찾도록 Siamese network를 훈련시킴\n",
    "- 두 input(search, exemplar)의 cross-correlation을 계산하여 dense하고 효율적인 sliding-window evaluation을 가능하도록 함\n",
    "\n",
    "### 2. Deep similiarity learning for tracking\n",
    "- x와 z의 similiarity 정도를 나타내는 $f(z, x)$를 학습하고, 이 값이 가장 큰 candidate를 선택한다. \n",
    "- object의 초기 appearance를 exemplar로 사용하며, f로 deep conv-net을 이용\n",
    "\n",
    "<img src='img/siamfc_1.png' width='200'>\n",
    "<img src='img/siamfc_2.png' width='400'>  \n",
    "> - g : simple distance or similiarity metric\n",
    "> - $\\psi$ : embedding\n",
    "> - \\* : cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2.1 Fully-convolutional Siamese architecture__\n",
    "- 두 이미지에 대한 embedding을 cross-correlation하는 부분을 아래과 같은 식으로 표현할 수 있음\n",
    "- Fully-convolutional의 장점은 더 큰 search image에 대해서 계산할 수 있다는 점이다.\n",
    "- network가 symmetric (f(x, z) = f(z, x))이기 때문에, 다른 크기의 exemplar image도 사용할 수 있다. \n",
    "- Output은 a score map이다  \n",
    "\n",
    "<img src='img/siamfc_3.png' width='150'>\n",
    "<img src='img/siamfc_4.png' width='150'>\n",
    "> - $L_T$ : translation operator  \n",
    "(여기서, translation이란 x, y축 방향으로 일정한 양만큼 이동시키는 과정)  \n",
    "> - $h$ : a function that maps signals to signals\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "- $f(z, x)$에 대한 정확한 식은 아래와 같다.  \n",
    "<img src='img/siamfc_5.png' width='200'>    \n",
    "\n",
    "\n",
    "- Tracking 시, 이전 프레임에서의 target의 위치를 중심점으로 한 search image를 이용하며, maximum score를 가지는 위치를 현재 프레임의 위치로 추정한다. \n",
    "\n",
    "### __2.2 Training__\n",
    "- Loss function으로 logistic loss를 이용하며, score map의 loss를 아래과 같이 정의한다.  \n",
    "<img src='img/siamfc_6.png' width='200'>\n",
    "<img src='img/siamfc_7.png' width='200'>   \n",
    "\n",
    "\n",
    "- SGD를 이용하여 이를 최소화하는 parameter를 학습한다.\n",
    "- 훈련 시, object의 class는 무시된다.\n",
    "- score map의 원소들은 아래와 같이 정의된다.   \n",
    "<img src='img/siamfc_8.png' width='200'>    \n",
    "\n",
    "\n",
    "- 클래스 불균형을 제거하기 위해 loss의 가중합을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5vtj3wTbbfM"
   },
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9nvobqubdKO"
   },
   "source": [
    "#### builder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqjaVnbFbeYM"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from .heads import Corr_Up, MultiRPN, DepthwiseRPN\n",
    "from .backbones import AlexNet, Vgg, ResNet22, Incep22, ResNeXt22, ResNet22W, resnet50, resnet34, resnet18\n",
    "from neck import AdjustLayer, AdjustAllLayer\n",
    "from .utils import load_pretrain\n",
    "\n",
    "__all__ = ['SiamFC_', 'SiamFC', 'SiamVGG', 'SiamFCRes22', 'SiamFCIncep22', 'SiamFCNext22', 'SiamFCRes22W',\n",
    "           'SiamRPN', 'SiamRPNVGG', 'SiamRPNRes22', 'SiamRPNIncep22', 'SiamRPNResNeXt22', 'SiamRPNPP']\n",
    "\n",
    "\n",
    "class SiamFC_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiamFC_, self).__init__()\n",
    "        self.features = None\n",
    "        # self.head = None\n",
    "\n",
    "    def head(self, z, x):\n",
    "        n, c, h, w = x.size()\n",
    "        x = x.view(1, n * c, h, w)\n",
    "        out = F.conv2d(x, z, groups=n)\n",
    "        out = out.view(n, 1, out.size(-2), out.size(-1))\n",
    "        return out\n",
    "\n",
    "    def feature_extractor(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "    def connector(self, template_feature, search_feature):\n",
    "        pred_score = self.head(template_feature, search_feature)\n",
    "        return pred_score\n",
    "\n",
    "    def branch(self, allin):\n",
    "        allout = self.feature_extractor(allin)\n",
    "        return allout\n",
    "\n",
    "    def forward(self, template, search):\n",
    "        zf = self.feature_extractor(template)\n",
    "        xf = self.feature_extractor(search)\n",
    "        score = self.connector(zf, xf)\n",
    "        return score\n",
    "\n",
    "\n",
    "class SiamFC(SiamFC_):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiamFC, self).__init__()\n",
    "        self.features = AlexNet()\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        zf = self.features(z)\n",
    "        xf = self.features(x)\n",
    "        score = self.head(zf, xf)\n",
    "        return score\n",
    "\n",
    "    def head(self, z, x):\n",
    "        # fast cross correlation\n",
    "        n, c, h, w = x.size()\n",
    "        x = x.view(1, n * c, h, w)\n",
    "        out = F.conv2d(x, z, groups=n)\n",
    "        out = out.view(n, 1, out.size(-2), out.size(-1))\n",
    "\n",
    "        # adjust the scale of responses\n",
    "        out = 0.001 * out + 0.0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight.data, mode='fan_out',\n",
    "                                     nonlinearity='relu')\n",
    "                m.bias.data.fill_(0)\n",
    "                \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Fyc9wLfC-eQ"
   },
   "source": [
    "### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKfk2HhaDClw"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from models.loss import *\n",
    "from image import load_data, generate_anchor, load_data_rpn\n",
    "from models.builder import *\n",
    "import dataset\n",
    "from mmcv import Config\n",
    "from utils import save_checkpoint, is_valid_number, bbox_iou\n",
    "from models.utils import load_pretrain\n",
    "from models.lr_scheduler import *\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch SiameseX')\n",
    "\n",
    "parser.add_argument('--config', metavar='model', default='configs/SiamRPN.py', type=str,\n",
    "                    help='which model to use.')\n",
    "\n",
    "parser.add_argument('--pre', '-p', metavar='PRETRAINED', default=None, type=str,\n",
    "                    help='path to the pretrained model')\n",
    "\n",
    "parser.add_argument('--gpu', metavar='GPU', default='0', type=str,\n",
    "                    help='GPU id to use.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWmNGMfTCuKT"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    global args, best_prec1, weight, segmodel\n",
    "    \n",
    "    best_prec1 = 0\n",
    "    prec1 = 0\n",
    "    coco = 0\n",
    "    temp_args = parser.parse_args()\n",
    "\n",
    "    args = Config.fromfile(temp_args.config)\n",
    "    args.pre = temp_args.pre\n",
    "    args.gpu = temp_args.gpu\n",
    "\n",
    "    with open('./data/ilsvrc_vid_new.txt', 'r') as outfile:\n",
    "        args.ilsvrc = json.load(outfile)\n",
    "    with open('./data/vot2018_new.txt', 'r') as outfile:\n",
    "        args.vot2018 = json.load(outfile)\n",
    "    if os.path.isfile('youtube_final_new.txt'):\n",
    "        with open('youtube_final_new.txt', 'r') as outfile:\n",
    "            args.youtube = json.load(outfile)\n",
    "    else:\n",
    "        args.youtube = None\n",
    "    # with open('vot2018.txt', 'r') as outfile:\n",
    "    #     args.vot2018 = json.load(outfile)\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu  # 해당 GPU에만 메모리를 할당\n",
    "                                        # 해주지 않으면 multi-GPU 시스템에서 모든 GPU에 메모리 할당\n",
    "    torch.cuda.manual_seed(args.seed)  # gpu 연산 randomness 고정\n",
    "\n",
    "    elif args.model == 'SiamFC':\n",
    "        model = SiamFC()\n",
    "\n",
    "    model = model.cuda()\n",
    "    model = model.eval()  # eval mode (batchnorm이나 dropout layer들이 eval mode로 사용되도록)\n",
    "\n",
    "\n",
    "    criterion = nn.SoftMarginLoss(size_average=False).cuda()  # for SiamFC and SiamVGG\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # cf) soft margin loss \n",
    "    #   : two-class classification logistic loss between input tensor x and target tensor y (1/-1)\n",
    "\n",
    "    #     - loss = sum_i (log(1 + exp(-y[i] * x[i])) / n)\n",
    "\n",
    "\n",
    "    # nn.SoftMarginLoss의 옵션\n",
    "    #   size_average=T : loss를 평균낼 것인지 (F면, summation)\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    if 'SiamRPNPP' in args.model:\n",
    "        optimizer, lr_scheduler = build_opt_lr(model, args.start_epoch)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                    momentum=args.momentum,\n",
    "                                    weight_decay=args.decay)\n",
    "\n",
    "    if args.pre:\n",
    "        if os.path.isfile(args.pre):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.pre))\n",
    "            checkpoint = torch.load(args.pre)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            args.start_epoch = 0\n",
    "\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            best_prec1 = 0\n",
    "            \n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.pre, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.pre))\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # cf) checkpoint\n",
    "    #   - state_dict : 모델의 매개변수를 담는 dictionary\n",
    "    #                  (model의 경우, weights들 & optimizer의 경우, lr와 같은 params)\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # prec1 = 0\n",
    "\n",
    "    if not os.path.isdir('./cp/temp'):\n",
    "        os.makedirs('./cp/temp')\n",
    "    \n",
    "    for epoch in range(args.start_epoch, args.epochs+1):\n",
    "\n",
    "        if 'SiamRPNPP' in args.model:\n",
    "            if args.backbone_train_epoch == epoch:\n",
    "                print('start training backbone.')\n",
    "                optimizer, lr_scheduler = build_opt_lr(model, epoch)\n",
    "\n",
    "            lr_scheduler.step(epoch)  # lr 갱신\n",
    "            cur_lr = lr_scheduler.get_cur_lr()\n",
    "\n",
    "        else:\n",
    "            cur_lr = adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        print('current learning rate : {}'.format(cur_lr))\n",
    "\n",
    "        if args.model in ['SiamFC', 'SiamVGG', 'SiamFCRes22', 'SiamFCIncep22', 'SiamFCNext22']:\n",
    "            train(model, criterion, optimizer, epoch, coco)\n",
    "        elif args.model in ['SiamRPN', 'SiamRPNVGG', 'SiamRPNRes22', 'SiamRPNIncep22', 'SiamRPNResNeXt22']:\n",
    "            trainRPN(model, optimizer, epoch, coco)\n",
    "        elif args.model in ['SiamRPNPP']:\n",
    "            trainRPNPP(model, optimizer, epoch, coco)\n",
    "\n",
    "        # is_best = False\n",
    "        \n",
    "        is_best = prec1 > best_prec1\n",
    "        \n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            torch.save(model.state_dict(), './cp/temp/{}_{}.pth'.format(args.model, epoch))\n",
    "        \n",
    "        # print(' * best MAE {mae:.3f} '\n",
    "        #       .format(mae=best_prec1))\n",
    "        # print(' * MAE {mae:.3f} '\n",
    "        #       .format(mae=prec1))\n",
    "\n",
    "        # best_prec1 보다 크면 현재 모델 save, 아니면 기존의 args.model save\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.pre,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, args.model)\n",
    "\n",
    "            \n",
    "def train(model, criterion, optimizer, epoch, coco):\n",
    "    losses = AverageMeter()   # Computes and stores the average and current value (using update)\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "\n",
    "    # listDataset을 이용하여 3개의 dataset를 load하는 dataloader 만들기\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset.listDataset(args.ilsvrc, args.youtube, args.data_type,\n",
    "                            shuffle=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                                        transforms.ToTensor(),\n",
    "                                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                             std=[0.229, 0.224, 0.225])]),\n",
    "                            train=True,\n",
    "                            batch_size=args.batch_size,\n",
    "                            num_workers=args.workers, coco=coco),\n",
    "        batch_size=args.batch_size)\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    \n",
    "    for i, (z, x, template, gt_box)in enumerate(train_loader):\n",
    "\n",
    "        # z : exemplar image\n",
    "        # x : search image\n",
    "        # template : 각 position에 대한 true label {1, -1}\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        z = z.cuda()\n",
    "        z = Variable(z)\n",
    "        x = x.cuda()\n",
    "        x = Variable(x)\n",
    "        template = template.type(torch.FloatTensor).cuda()  \n",
    "        template = Variable(template)\n",
    "        \n",
    "        oup = model(z, x)\n",
    "\n",
    "        if isinstance(model, SiamFC) or isinstance(model, SiamVGG):\n",
    "            loss = criterion(oup, template)\n",
    "        elif isinstance(model, SiamFCRes22):\n",
    "            loss = model.train_loss(oup, template)\n",
    "\n",
    "        losses.update(loss.item(), x.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if isinstance(model, SiamFCRes22):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10)  # gradient clip\n",
    "\n",
    "        if is_valid_number(loss.item()):\n",
    "            optimizer.step()  # 파라미터 update\n",
    "\n",
    "        # optimizer.step()\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "\n",
    "\n",
    "LRs = {\n",
    "    'log': LogScheduler,\n",
    "    'step': StepScheduler,\n",
    "    'multi-step': MultiStepScheduler,\n",
    "    'linear': LinearStepScheduler,\n",
    "    'cos': CosStepScheduler}\n",
    "\n",
    "\n",
    "def _build_lr_scheduler(optimizer, lr_type, epochs=50, last_epoch=-1):\n",
    "    return LRs[lr_type](optimizer, last_epoch=last_epoch,\n",
    "                            epochs=epochs, new_allowed=True)\n",
    "\n",
    "\n",
    "def _build_warm_up_scheduler(optimizer, epochs=50, last_epoch=-1):\n",
    "    warmup_epoch = args.lr_warm_epoch\n",
    "    sc1 = _build_lr_scheduler(optimizer, 'step', warmup_epoch, last_epoch)\n",
    "    sc2 = _build_lr_scheduler(optimizer, 'log', epochs - warmup_epoch, last_epoch)\n",
    "    return WarmUPScheduler(optimizer, sc1, sc2, epochs, last_epoch)\n",
    "\n",
    "\n",
    "def build_lr_scheduler(optimizer, epochs=50, last_epoch=-1):\n",
    "    if args.warmup:\n",
    "        return _build_warm_up_scheduler(optimizer, epochs, last_epoch)\n",
    "    else:\n",
    "        return _build_lr_scheduler(optimizer, args.original_lr, epochs, last_epoch)\n",
    "\n",
    "\n",
    "def build_opt_lr(model, current_epoch=0):\n",
    "    if current_epoch >= 20:\n",
    "        for layer in ['layer2', 'layer3', 'layer4']:\n",
    "            for param in getattr(model.features, layer).parameters():\n",
    "                param.requires_grad = True\n",
    "            for m in getattr(model.features, layer).modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.train()\n",
    "    else:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        for m in model.features.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    trainable_params = []\n",
    "    trainable_params += [{'params': filter(lambda x: x.requires_grad,\n",
    "                                           model.features.parameters()),\n",
    "                          'lr': 0.1 * args.original_lr}]\n",
    "\n",
    "    trainable_params += [{'params': model.neck.parameters(),\n",
    "                        'lr': args.original_lr}]\n",
    "\n",
    "    trainable_params += [{'params': model.head.parameters(),\n",
    "                          'lr': args.original_lr}]\n",
    "\n",
    "    optimizer = torch.optim.SGD(trainable_params,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.decay)\n",
    "\n",
    "    lr_scheduler = build_lr_scheduler(optimizer, epochs=args.epochs)\n",
    "    lr_scheduler.step(args.start_epoch)\n",
    "    return optimizer, lr_scheduler\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6_3_6YSbaAb"
   },
   "source": [
    "### demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xba6pntC95z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import demo_utils.vot as vot\n",
    "from demo_utils.siamvggtracker import SiamVGGTracker\n",
    "\n",
    "\n",
    "# *****************************************\n",
    "# VOT: Create VOT handle at the beginning\n",
    "#      Then get the initializaton region\n",
    "#      and the first image\n",
    "# *****************************************\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch SiameseX demo')\n",
    "\n",
    "parser.add_argument('--model', metavar='model', default='SiamFCNext22', type=str,\n",
    "                    help='which model to use.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "handle = vot.VOT(\"rectangle\")\n",
    "selection = handle.region()\n",
    "\n",
    "# Process the first frame\n",
    "imagefile = handle.frame()  # get frame from client\n",
    "tracker = SiamVGGTracker(args.model, imagefile, selection)\n",
    "\n",
    "if not imagefile:\n",
    "    sys.exit(0)\n",
    "\n",
    "toc = 0\n",
    "\n",
    "while True:\n",
    "    # *****************************************\n",
    "    # VOT: Call frame method to get path of the \n",
    "    #      current image frame. If the result is\n",
    "    #      null, the sequence is over.\n",
    "    # *****************************************\n",
    "\n",
    "    tic = cv2.getTickCount()  # 연산시간을 구하기 위해 tick 횟수 차이를 구하고, 틱 주파수를 나눠줘야 함\n",
    "\n",
    "    imagefile = handle.frame()\n",
    "    image = cv2.imread(imagefile)\n",
    "    if not imagefile:\n",
    "        break\n",
    "    region, confidence = tracker.track(imagefile)\n",
    "    toc += cv2.getTickCount() - tic\n",
    "\n",
    "    region = vot.Rectangle(region.x, region.y, region.width, region.height)\n",
    "    \n",
    "    # *****************************************\n",
    "    # VOT: Report the position of the object\n",
    "    #      every frame using report method.\n",
    "    # *****************************************\n",
    "    handle.report(region, confidence)\n",
    "    cv2.rectangle(image, (int(region.x), int(region.y)), (int(region.x + region.width), int(region.y + region.height)), (0, 255, 255), 3)\n",
    "    cv2.imshow('SiameseX', image)\n",
    "    cv2.waitKey(1)\n",
    "    # if cv2.waitKey() == 27:\n",
    "    #     break\n",
    "\n",
    "    print('Tracking Speed {:.1f}fps'.format((len(handle) - 1) / (toc / cv2.getTickFrequency())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
