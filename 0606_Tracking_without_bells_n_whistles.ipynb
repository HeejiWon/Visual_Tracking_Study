{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ7IKS5XyXYN"
   },
   "source": [
    "## Tracking without bells and whistles\n",
    "- Tracking without bells and whistles의 Tracktor.py에 해당하는 코드\n",
    "- paper : https://arxiv.org/pdf/1903.05625.pdf\n",
    "- code : https://github.com/phil-bergmann/tracking_wo_bnw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgY8LpNSrsbT"
   },
   "source": [
    "#### Tracktor에 대한 설명\n",
    "\n",
    "- for t=0, detection의 결과로 tracker intialize\n",
    "- for t>0, 2 step :   \n",
    "    ○ __bbox regression__(아래 그림에서 파란색 arrow)  \n",
    "    ○ __track intialization__(아래 그림에서 빨간색 arrow)\n",
    "<img src='img/bell_whistles.png' width='600'>\n",
    "\n",
    "#### Bouding box regression\n",
    "- __$b^k_{t-1}$ 를 이용하여 $b_t^k$ 를 regression__\n",
    "- Faster R-CNN의 경우에는, __이전 프레임 좌표에 대한 RoI pooling__을 현재 프레임에 대하여 적용 (가정 : high frame rates에 의해 target이 약간만 움직였을 것)\n",
    "- 그래서 자동으로 __ID가 현재 프레임으로 이동__할 것이고, 이로 인해 짧은 trajectory가 만들어진다.\n",
    "- tracjectory가 deactivating되는 두 가지 경우  \n",
    "    ○ object가 프레임을 벗어나거나 non-object에 의해 가려지는 경우  \n",
    "    ○ object간의 occlusion가 일어났을 때, 이에 대해  NMS를 적용하였더니 bouding box가 삭제되는 경우\n",
    "\n",
    "#### Bounding box initialization\n",
    "- 이전 프레임에 등장하지 않았던 __새로운 target__ 또한 고려하기 위해 t시점의 전체 프레임에 대한 __detection $D_t$__ 가 주어진다. \n",
    "- __$D_t$  중 active trajectories $b_t^k$ 와의 IoU가 \\$lambda_new\\$ 보다 작은 경우에만 initialize__ (즉, 어떤 trajectory로도 설명되지 않는 새로운 tracklet를 만들어주기 위함)\n",
    "- 주목할 점은 어떠한 tracking specific training이나 optimization을 필요로 하지 않고 단순히 object detection method에만 영향을 받는다는 점이다 (따라서 다른 dataset이나 scenarios에도 쉽게 적용가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import cv2\n",
    "\n",
    "from .utils import bbox_overlaps, warp_pos, get_center, get_height, get_width, make_pos\n",
    "\n",
    "from torchvision.ops.boxes import clip_boxes_to_image, nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDL9fdV5hcMt"
   },
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    \"\"\"The main tracking file, here is where magic happens.\"\"\"\n",
    "    # only track pedestrian\n",
    "    cl = 1\n",
    "\n",
    "    def __init__(self, obj_detect, reid_network, tracker_cfg):\n",
    "        self.obj_detect = obj_detect  # detection model  (Faster R-CNN 이용)\n",
    "        self.reid_network = reid_network  # Siamese CNN (official에서는 resnet 이용)\n",
    "        self.detection_person_thresh = tracker_cfg['detection_person_thresh']\n",
    "        self.regression_person_thresh = tracker_cfg['regression_person_thresh']\n",
    "        self.detection_nms_thresh = tracker_cfg['detection_nms_thresh']\n",
    "        self.regression_nms_thresh = tracker_cfg['regression_nms_thresh']\n",
    "        self.public_detections = tracker_cfg['public_detections']  # 공개 데이터셋 여부 (특정 모델의 result(pred bbox)를 들고 올 수 있는지)\n",
    "        self.inactive_patience = tracker_cfg['inactive_patience']  # inactive되는 기간이 inactive patience 보다 길면 remove\n",
    "        self.do_reid = tracker_cfg['do_reid']  # re-ID를 수행 여부 (T/F)\n",
    "        self.max_features_num = tracker_cfg['max_features_num']  # Track에서 고려할 feature 수\n",
    "        self.reid_sim_threshold = tracker_cfg['reid_sim_threshold']  # re-ID를 위한 similiarity measure (distance)에 대한 threshold \n",
    "        self.reid_iou_threshold = tracker_cfg['reid_iou_threshold']  # re-ID를 위한 iou threshold\n",
    "        self.do_align = tracker_cfg['do_align']  # ECC(Enhanced Correlation Coefficient)를 사용한 motion compensation 적용 여부\n",
    "                                                 # 사용하지 않으면 적은 수치의 등속으로 가정\n",
    "        self.motion_model_cfg = tracker_cfg['motion_model']  # moction compensation 사용여부\n",
    "\n",
    "\n",
    "        self.warp_mode = getattr(cv2, tracker_cfg['warp_mode'])\n",
    "        self.number_of_iterations = tracker_cfg['number_of_iterations']\n",
    "        self.termination_eps = tracker_cfg['termination_eps']\n",
    "\n",
    "        self.tracks = []  # active tracks\n",
    "        self.inactive_tracks = []  # inactive tracks\n",
    "        self.track_num = 0  # 지금까지 만들어진 track 수 \n",
    "        self.im_index = 0  # image index\n",
    "        self.results = {}\n",
    "\n",
    "    # reset tracks\n",
    "    def reset(self, hard=True):\n",
    "        self.tracks = []  # tracker 초기화\n",
    "        self.inactive_tracks = []\n",
    "\n",
    "        if hard:\n",
    "            self.track_num = 0\n",
    "            self.results = {}\n",
    "            self.im_index = 0\n",
    "\n",
    "    # Make inactive tracks list \n",
    "    def tracks_to_inactive(self, tracks):\n",
    "        self.tracks = [t for t in self.tracks if t not in tracks]\n",
    "        for t in tracks:\n",
    "            t.pos = t.last_pos[-1]\n",
    "        self.inactive_tracks += tracks\n",
    "\n",
    "    # Add New tracks\n",
    "    def add(self, new_det_pos, new_det_scores, new_det_features):\n",
    "        \"\"\"Initializes new Track objects and saves them.\"\"\"\n",
    "        num_new = new_det_pos.size(0)  # size : 몇 개의 요소가 있는지 ex) 3*4인 2차원 array -> 12\n",
    "        for i in range(num_new):\n",
    "            self.tracks.append(Track(\n",
    "                new_det_pos[i].view(1, -1),\n",
    "                new_det_scores[i],\n",
    "                self.track_num + i,\n",
    "                new_det_features[i].view(1, -1),\n",
    "                self.inactive_patience,\n",
    "                self.max_features_num,\n",
    "                self.motion_model_cfg['n_steps'] if self.motion_model_cfg['n_steps'] > 0 else 1\n",
    "            ))\n",
    "        self.track_num += num_new\n",
    "\n",
    "    def regress_tracks(self, blob):\n",
    "        \"\"\"\n",
    "        Regress the position of the tracks and also checks their scores.\n",
    "        \"\"\"\n",
    "        pos = self.get_pos()  # active track에 있는 positions (이전 프레임의 position) 가져오기\n",
    "\n",
    "        # regress\n",
    "        boxes, scores = self.obj_detect.predict_boxes(pos)  # Facter RCNN의 RoI head 부분 (RoI Pooling하고, Bbox regression & Classification)\n",
    "        pos = clip_boxes_to_image(boxes, blob['img'].shape[-2:])    # box가 image안에 포함되도록 image를 벗어나는 x, y 좌표 clamp해주기\n",
    " \n",
    "        s = []\n",
    "        for i in range(len(self.tracks) - 1, -1, -1):   # 뒤에서부터\n",
    "            t = self.tracks[i]\n",
    "            t.score = scores[i]\n",
    "\n",
    "            # score가 threshold보다 작으면 inactive (self.tracks에서 빼주고 inactive_tracks에 넣어주기)\n",
    "            if scores[i] <= self.regression_person_thresh:  \n",
    "                self.tracks_to_inactive([t])\n",
    "            \n",
    "            # score가 threshold보다 크면\n",
    "            else:\n",
    "                s.append(scores[i])\n",
    "                t.pos = pos[i].view(1, -1)\n",
    "\n",
    "        return torch.Tensor(s[::-1]).cuda()  # 나중에 들어온 track에 대해서 먼저 score를 넣어줬기 때문에 list reverse\n",
    "\n",
    "    def get_pos(self):\n",
    "        \"\"\"Get the positions of all active tracks.\"\"\"\n",
    "        if len(self.tracks) == 1:\n",
    "            pos = self.tracks[0].pos\n",
    "        elif len(self.tracks) > 1:\n",
    "            pos = torch.cat([t.pos for t in self.tracks], 0)\n",
    "        else:\n",
    "            pos = torch.zeros(0).cuda()\n",
    "        return pos\n",
    "\n",
    "    def get_features(self):\n",
    "        \"\"\"Get the features of all active tracks.\"\"\"\n",
    "        if len(self.tracks) == 1:\n",
    "            features = self.tracks[0].features\n",
    "        elif len(self.tracks) > 1:\n",
    "            features = torch.cat([t.features for t in self.tracks], 0)\n",
    "        else:\n",
    "            features = torch.zeros(0).cuda()\n",
    "        return features\n",
    "\n",
    "    def get_inactive_features(self):\n",
    "        \"\"\"Get the features of all inactive tracks.\"\"\"\n",
    "        if len(self.inactive_tracks) == 1:\n",
    "            features = self.inactive_tracks[0].features\n",
    "        elif len(self.inactive_tracks) > 1:\n",
    "            features = torch.cat([t.features for t in self.inactive_tracks], 0)\n",
    "        else:\n",
    "            features = torch.zeros(0).cuda()\n",
    "        return features\n",
    "\n",
    "    def reid(self, blob, new_det_pos, new_det_scores):\n",
    "        \"\"\"Tries to ReID inactive tracks with new detections.\"\"\"\n",
    "        new_det_features = [torch.zeros(0).cuda() for _ in range(len(new_det_pos))]\n",
    "\n",
    "        if self.do_reid:\n",
    "            # new_det_pos의 appearance feature 구하기\n",
    "            # 이때 blob['img'] (BS, c, h, w)에 대해서 net_det_pos 부분을 crop해서 reid network에 forward 시킴\n",
    "            new_det_features = self.reid_network.test_rois(\n",
    "                blob['img'], new_det_pos).data\n",
    "\n",
    "            # inactive track과의 reID (active track과 matching된 게 없으므로)\n",
    "            if len(self.inactive_tracks) >= 1:\n",
    "\n",
    "                # calculate appearance distances\n",
    "                dist_mat, pos = [], []\n",
    "                for t in self.inactive_tracks:\n",
    "\n",
    "                    # 기존 (inactive) track과의 appearance distance matrix\n",
    "                    dist_mat.append(torch.cat([t.test_features(feat.view(1, -1))   \n",
    "                                               for feat in new_det_features], dim=1))\n",
    "                    pos.append(t.pos)\n",
    "\n",
    "                if len(dist_mat) > 1:\n",
    "                    dist_mat = torch.cat(dist_mat, 0)\n",
    "                    pos = torch.cat(pos, 0)\n",
    "                else:\n",
    "                    dist_mat = dist_mat[0]\n",
    "                    pos = pos[0]\n",
    "\n",
    "                # calculate IoU distance (IoU가 threshold보다 작은 값들에 대해 dist_mat 행렬에 큰 값을 부여하기 위해, \n",
    "                # 즉 거리를 멀게 하여 assignment를 어렵게 하기 위해)\n",
    "                iou = bbox_overlaps(pos, new_det_pos)\n",
    "                iou_mask = torch.ge(iou, self.reid_iou_threshold)\n",
    "                iou_neg_mask = ~iou_mask\n",
    "\n",
    "                # make all impossible assignments to the same add big value\n",
    "                dist_mat = dist_mat * iou_mask.float() + iou_neg_mask.float() * 1000  # iou가 threshold보다 작은 값들에 대하여 (iou_neg_mask) 1000를 더해줌\n",
    "                dist_mat = dist_mat.cpu().numpy()\n",
    "\n",
    "                row_ind, col_ind = linear_sum_assignment(dist_mat)  # Hungarian algorithm을 이용해 assignment 해주기\n",
    "                # row : inactive_tracks, col : new_det_pos\n",
    "\n",
    "                # row_ind, col_ind를 이용하여 row_ind에 해당하는 inactive_trackers를 self.tracks에 넣어주고,\n",
    "                # position update, 새로 계산한 new_det_feature를 features에 add 해주기\n",
    "                assigned = []\n",
    "                remove_inactive = []\n",
    "                for r, c in zip(row_ind, col_ind):\n",
    "                    if dist_mat[r, c] <= self.reid_sim_threshold:\n",
    "                        t = self.inactive_tracks[r]\n",
    "                        self.tracks.append(t)\n",
    "                        t.count_inactive = 0\n",
    "                        t.pos = new_det_pos[c].view(1, -1)\n",
    "                        t.reset_last_pos()\n",
    "                        t.add_features(new_det_features[c].view(1, -1))\n",
    "                        assigned.append(c)\n",
    "                        remove_inactive.append(t)\n",
    "\n",
    "                for t in remove_inactive:\n",
    "                    self.inactive_tracks.remove(t)\n",
    "\n",
    "                # re-ID되지 않은, 즉 처음으로 나온 object를 keep해서 이것을 리턴해줌\n",
    "                keep = torch.Tensor([i for i in range(new_det_pos.size(0)) if i not in assigned]).long().cuda()\n",
    "                if keep.nelement() > 0:\n",
    "                    new_det_pos = new_det_pos[keep]\n",
    "                    new_det_scores = new_det_scores[keep]\n",
    "                    new_det_features = new_det_features[keep]\n",
    "                else:\n",
    "                    new_det_pos = torch.zeros(0).cuda()\n",
    "                    new_det_scores = torch.zeros(0).cuda()\n",
    "                    new_det_features = torch.zeros(0).cuda()\n",
    "\n",
    "        return new_det_pos, new_det_scores, new_det_features\n",
    "\n",
    "    def get_appearances(self, blob):\n",
    "        \"\"\"Uses the siamese CNN to get the features for all active tracks.\"\"\"\n",
    "        new_features = self.reid_network.test_rois(blob['img'], self.get_pos()).data\n",
    "        return new_features\n",
    "\n",
    "    def add_features(self, new_features):\n",
    "        \"\"\"Adds new appearance features to active tracks.\"\"\"\n",
    "        for t, f in zip(self.tracks, new_features):\n",
    "            t.add_features(f.view(1, -1))\n",
    "\n",
    "    def align(self, blob):\n",
    "        \"\"\"Aligns the positions of active and inactive tracks depending on camera motion.\"\"\"\n",
    "        if self.im_index > 0:\n",
    "            im1 = np.transpose(self.last_image.cpu().numpy(), (1, 2, 0))\n",
    "            im2 = np.transpose(blob['img'][0].cpu().numpy(), (1, 2, 0))\n",
    "            im1_gray = cv2.cvtColor(im1, cv2.COLOR_RGB2GRAY)\n",
    "            im2_gray = cv2.cvtColor(im2, cv2.COLOR_RGB2GRAY)\n",
    "            warp_matrix = np.eye(2, 3, dtype=np.float32)\n",
    "            criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, self.number_of_iterations,  self.termination_eps)\n",
    "            cc, warp_matrix = cv2.findTransformECC(im1_gray, im2_gray, warp_matrix, self.warp_mode, criteria)\n",
    "            warp_matrix = torch.from_numpy(warp_matrix)\n",
    "\n",
    "            for t in self.tracks:\n",
    "                t.pos = warp_pos(t.pos, warp_matrix)\n",
    "                # t.pos = clip_boxes(Variable(pos), blob['im_info'][0][:2]).data\n",
    "\n",
    "            if self.do_reid:\n",
    "                for t in self.inactive_tracks:\n",
    "                    t.pos = warp_pos(t.pos, warp_matrix)\n",
    "\n",
    "            if self.motion_model_cfg['enabled']:\n",
    "                for t in self.tracks:\n",
    "                    for i in range(len(t.last_pos)):\n",
    "                        t.last_pos[i] = warp_pos(t.last_pos[i], warp_matrix)\n",
    "\n",
    "    def motion_step(self, track):\n",
    "        \"\"\"Updates the given track's position by one step based on track.last_v\"\"\"\n",
    "        if self.motion_model_cfg['center_only']:\n",
    "            center_new = get_center(track.pos) + track.last_v\n",
    "            track.pos = make_pos(*center_new, get_width(track.pos), get_height(track.pos))\n",
    "        else:\n",
    "            track.pos = track.pos + track.last_v\n",
    "\n",
    "    def motion(self):\n",
    "        \"\"\"Applies a simple linear motion model that considers the last n_steps steps.\"\"\"\n",
    "        for t in self.tracks:\n",
    "            last_pos = list(t.last_pos)\n",
    "\n",
    "            # avg velocity between each pair of consecutive positions in t.last_pos\n",
    "            if self.motion_model_cfg['center_only']:\n",
    "                vs = [get_center(p2) - get_center(p1) for p1, p2 in zip(last_pos, last_pos[1:])]\n",
    "            else:\n",
    "                vs = [p2 - p1 for p1, p2 in zip(last_pos, last_pos[1:])]\n",
    "\n",
    "            t.last_v = torch.stack(vs).mean(dim=0)  \n",
    "            self.motion_step(t)\n",
    "\n",
    "        if self.do_reid:\n",
    "            for t in self.inactive_tracks:\n",
    "                if t.last_v.nelement() > 0:\n",
    "                    self.motion_step(t)\n",
    "\n",
    "\n",
    "    def step(self, blob):\n",
    "        \"\"\"\n",
    "        매 시점마다 실행되는 함수\n",
    "\n",
    "        This function should be called every timestep to perform tracking with a blob\n",
    "        containing the image information.\n",
    "        \"\"\"\n",
    "\n",
    "        # 각 track들의 현재 pos를 last_pos에 append\n",
    "        for t in self.tracks:\n",
    "            t.last_pos.append(t.pos.clone())\n",
    "\n",
    "        ###########################\n",
    "        # Look for new detections #\n",
    "        ###########################\n",
    "        '''\n",
    "        공개된 데이터셋이라면 그 결과(detection 결과)를 이용하고 아니라면 detection 진행\n",
    "        detect된 box가 있다면 해당 box와 score를 det_pos, det_score로 설정\n",
    "        '''\n",
    "\n",
    "        self.obj_detect.load_image(blob['img'])\n",
    "\n",
    "        # 공개된 데이터셋이면 결과가 있으므로 그 결과를 이용\n",
    "        if self.public_detections:\n",
    "            dets = blob['dets'].squeeze(dim=0)  # dets를 가져옴\n",
    "            \n",
    "            # dets가 있으면, RPN(Region Proposal Network)를 진행하지 않고 bbox regression과 classification만 진행\n",
    "            if dets.nelement() > 0:\n",
    "                boxes, scores = self.obj_detect.predict_boxes(dets)\n",
    "            # dets가 없으면 boxes와 scores를 빈 텐서로 만들어줌\n",
    "            else:\n",
    "                boxes = scores = torch.zeros(0).cuda()\n",
    "\n",
    "        # 공개된 데이터셋이 아니면 detection 진행\n",
    "        else:\n",
    "            boxes, scores = self.obj_detect.detect(blob['img'])\n",
    "\n",
    "\n",
    "        # boxes가 있다면 image안에 들어오도록 좌표를 clamp 해줌\n",
    "        if boxes.nelement() > 0:\n",
    "            boxes = clip_boxes_to_image(boxes, blob['img'].shape[-2:])\n",
    "\n",
    "            # Filter out tracks that have too low person score\n",
    "            inds = torch.gt(scores, self.detection_person_thresh).nonzero().view(-1)  \n",
    "                # torch.gt : Computes \\text{input} > \\text{other}input>other element-wise.\n",
    "        else:\n",
    "            inds = torch.zeros(0).cuda()\n",
    "\n",
    "        # inds가 있다면 해당 boxes, score를 det_pos, det_scores로 설정\n",
    "        if inds.nelement() > 0:\n",
    "            det_pos = boxes[inds]\n",
    "            det_scores = scores[inds]\n",
    "        else:\n",
    "            det_pos = torch.zeros(0).cuda()\n",
    "            det_scores = torch.zeros(0).cuda()\n",
    "\n",
    "        ##################\n",
    "        # Predict tracks #\n",
    "        ##################\n",
    "        '''\n",
    "        카메라 Motion 기반 조정 및 box regression\n",
    "        '''\n",
    "\n",
    "        num_tracks = 0\n",
    "        nms_inp_reg = torch.zeros(0).cuda()\n",
    "\n",
    "        # tracks이 하나라도 있으면\n",
    "        if len(self.tracks):\n",
    "\n",
    "            # align \n",
    "            # 카메라 모션에 따라 active/inactive tracks의 position을 align\n",
    "            if self.do_align:\n",
    "                self.align(blob)\n",
    "\n",
    "            # apply motion model\n",
    "            if self.motion_model_cfg['enabled']:\n",
    "                self.motion()\n",
    "                self.tracks = [t for t in self.tracks if t.has_positive_area()]  # area가 양수인 것만 self.tracks로!\n",
    "\n",
    "            # regress\n",
    "            person_scores = self.regress_tracks(blob)\n",
    "\n",
    "            if len(self.tracks):\n",
    "                # NMS를 통해 keep할 track/inactive track 분리하기\n",
    "                keep = nms(self.get_pos(), person_scores, self.regression_nms_thresh)\n",
    "                self.tracks_to_inactive([self.tracks[i] for i in list(range(len(self.tracks))) if i not in keep])\n",
    "\n",
    "                # do_reid 이면 appearance vector 구해서 track에 feature 넣어주기\n",
    "                if keep.nelement() > 0 and self.do_reid:\n",
    "                        new_features = self.get_appearances(blob)\n",
    "                        self.add_features(new_features)\n",
    "\n",
    "        #####################\n",
    "        # Create new tracks #\n",
    "        #####################\n",
    "\n",
    "        # 새로운 트랙이 생성될 때 (이전에 트랙한 object가 아니라고 판단 되었을 때)\n",
    "\n",
    "        # !!! Here NMS is used to filter out detections that are already covered by tracks. This is\n",
    "        # !!! done by iterating through the active tracks one by one, assigning them a bigger score\n",
    "        # !!! than 1 (maximum score for detections) and then filtering the detections with NMS.\n",
    "        # !!! In the paper this is done by calculating the overlap with existing tracks, but the\n",
    "        # !!! result stays the same.\n",
    "\n",
    "        # detection된 것이 하나라도 있으면 track들과 NMS 해주기\n",
    "        if det_pos.nelement() > 0:\n",
    "            keep = nms(det_pos, det_scores, self.detection_nms_thresh)\n",
    "            det_pos = det_pos[keep]\n",
    "            det_scores = det_scores[keep]\n",
    "\n",
    "            # 각 track에 대하여 det_pos와 비교\n",
    "            # check with every track in a single run (problem if tracks delete each other)\n",
    "            for t in self.tracks:\n",
    "                nms_track_pos = torch.cat([t.pos, det_pos])  # track의 position과 det의 position concatenate\n",
    "                nms_track_scores = torch.cat(\n",
    "                    [torch.tensor([2.0]).to(det_scores.device), det_scores]) \n",
    "                keep = nms(nms_track_pos, nms_track_scores, self.detection_nms_thresh)  # nms : return the indices\n",
    "\n",
    "                keep = keep[torch.ge(keep, 1)] - 1   # nms_track_pos에는 0번째 인덱스에 t.pos가 있기 때문에\n",
    "                                                     # det_pos에서 keep할 index를 구해주기 위해서 \n",
    "                                                     # 1보다 큰 keep(index)에 대해서 1 빼주기\n",
    "                                                     # torch.ge : Computes input(왼)≥other(오) element-wise.        \n",
    "                det_pos = det_pos[keep]\n",
    "                det_scores = det_scores[keep]\n",
    "                if keep.nelement() == 0:\n",
    "                    break\n",
    "\n",
    "        # 모든 track과의 nms 적용 후, 살아남은 det_pos가 있다면 initialize\n",
    "        if det_pos.nelement() > 0:\n",
    "            new_det_pos = det_pos\n",
    "            new_det_scores = det_scores\n",
    "\n",
    "            # re-ID matching을 해주고 matching되지 않은 (즉, 처음 나온) object를 리턴\n",
    "            new_det_pos, new_det_scores, new_det_features = self.reid(blob, new_det_pos, new_det_scores)\n",
    "\n",
    "            # 새로운 object에 대한 tracker를 만들어주고 self.tracks에 추가해줌\n",
    "            if new_det_pos.nelement() > 0:\n",
    "                self.add(new_det_pos, new_det_scores, new_det_features)\n",
    "\n",
    "        ####################\n",
    "        # Generate Results #\n",
    "        ####################\n",
    "\n",
    "        for t in self.tracks:\n",
    "            if t.id not in self.results.keys():\n",
    "                self.results[t.id] = {}\n",
    "            self.results[t.id][self.im_index] = np.concatenate([t.pos[0].cpu().numpy(), np.array([t.score])])\n",
    "            # 즉, t.id : {\"im_index\" : [t.pos[0].cpu().numpy(), np.array([t.score])], ...}\n",
    "\n",
    "        # inactive track의 count_inactive +1 해주기\n",
    "        for t in self.inactive_tracks:\n",
    "            t.count_inactive += 1\n",
    "\n",
    "        # 정해준 임계값 (inactive_patience) 보다 count_inactive 큰 경우는 remove\n",
    "        self.inactive_tracks = [\n",
    "            t for t in self.inactive_tracks if t.has_positive_area() and t.count_inactive <= self.inactive_patience\n",
    "        ]\n",
    "\n",
    "        self.im_index += 1  # image index\n",
    "        self.last_image = blob['img'][0]\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "\n",
    "class Track(object):\n",
    "    \"\"\"This class contains all necessary for every individual track.\"\"\"\n",
    "\n",
    "    def __init__(self, pos, score, track_id, features, inactive_patience, max_features_num, mm_steps):\n",
    "        self.id = track_id\n",
    "        self.pos = pos\n",
    "        self.score = score\n",
    "        self.features = deque([features])\n",
    "        self.ims = deque([])\n",
    "        self.count_inactive = 0\n",
    "        self.inactive_patience = inactive_patience\n",
    "        self.max_features_num = max_features_num\n",
    "        self.last_pos = deque([pos.clone()], maxlen=mm_steps + 1)\n",
    "        self.last_v = torch.Tensor([])\n",
    "        self.gt_id = None\n",
    "\n",
    "    def has_positive_area(self):\n",
    "        return self.pos[0, 2] > self.pos[0, 0] and self.pos[0, 3] > self.pos[0, 1]\n",
    "\n",
    "    def add_features(self, features):\n",
    "        \"\"\"Adds new appearance features to the object.\"\"\"\n",
    "        self.features.append(features)\n",
    "        if len(self.features) > self.max_features_num:\n",
    "            self.features.popleft()\n",
    "\n",
    "    def test_features(self, test_features):\n",
    "        \"\"\"Compares test_features to features of this Track object\"\"\"\n",
    "        if len(self.features) > 1:\n",
    "            features = torch.cat(list(self.features), dim=0)\n",
    "        else:\n",
    "            features = self.features[0]\n",
    "        features = features.mean(0, keepdim=True)\n",
    "        dist = F.pairwise_distance(features, test_features, keepdim=True)\n",
    "        return dist\n",
    "\n",
    "    def reset_last_pos(self):\n",
    "        self.last_pos.clear()\n",
    "        self.last_pos.append(self.pos.clone())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPNvTgNSx7dn3mKJJQoGXJw",
   "collapsed_sections": [],
   "name": "0606_Tracking_without_bells_n_whistles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
